{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e77268ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f8d0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    string = string.lower()\n",
    "    string = (unicodedata.normalize('NFKD', string)\n",
    "                         .encode('ascii', 'ignore')\n",
    "                         .decode('utf-8', 'ignore')\n",
    "             )\n",
    "    string = re.sub(r\"[^a-z0-9'\\s]\", '', string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf43614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(string):\n",
    "    string = re.sub(r'<[^>]*>', '', string)\n",
    "    string = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", '', string)\n",
    "    string = re.sub(r'\\n', '', string)\n",
    "    string = re.sub(r'\\s\\s', '', string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98d0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    return tokenizer.tokenize(string, return_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c048c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59360c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(string):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c91f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words=[], exclude_words=[]):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    for word in extra_words:\n",
    "        stopword_list.append(word)\n",
    "    \n",
    "    for word in exclude_words:\n",
    "        stopword_list.remove(word)\n",
    "        \n",
    "    words = string.split()\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d285e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_readme_data(df,column):\n",
    "    clean_tokens = (df[column].apply(clean_html)\n",
    "                              .apply(basic_clean)\n",
    "                              .apply(tokenize)\n",
    "                              .apply(remove_stopwords)\n",
    "                   )\n",
    "    \n",
    "    for token in clean_tokens:\n",
    "        token = ' '.join(token).split()\n",
    "    \n",
    "    df['stemmed'] = clean_tokens.apply(stem)\n",
    "    df['lemmatized'] = clean_tokens.apply(lemma)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d121fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_data():\n",
    "    data = pd.read_json('data.json')\n",
    "    return prepare_readme_data(data, 'readme_contents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c81be840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rdpeng/ExData_Plotting1</td>\n",
       "      <td>None</td>\n",
       "      <td>## Introduction\\n\\nThis assignment uses data f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rdpeng/RepData_PeerAssessment1</td>\n",
       "      <td>None</td>\n",
       "      <td>## Introduction\\n\\nIt is now possible to colle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DataScienceSpecialization/courses</td>\n",
       "      <td>HTML</td>\n",
       "      <td>\\n### Data Science Specialization\\n\\nThese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fighting41love/funNLP</td>\n",
       "      <td>Python</td>\n",
       "      <td>&lt;center&gt;\\n    &lt;img style=\"border-radius: 0.312...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>magento/magento2</td>\n",
       "      <td>PHP</td>\n",
       "      <td>\\n&lt;p align=\"center\"&gt;\\n&lt;a href=\"https://www.cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>rprokap/pset-9</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># pset-9\\n      CREDITS SEQUENCE              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>konzy/mass_clone</td>\n",
       "      <td>Shell</td>\n",
       "      <td># mass_clone\\nThis is a shell script that will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>biter777/countries</td>\n",
       "      <td>Go</td>\n",
       "      <td># countries\\r\\n\\r\\nCountries - ISO 3166 (ISO31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>sayantann11/all-classification-templetes-for-ML</td>\n",
       "      <td>Python</td>\n",
       "      <td># all-classification-templetes-for-ML\\nClassif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>denman2328/Help</td>\n",
       "      <td>None</td>\n",
       "      <td>------------------\\nSystem Information\\n------...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>490 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                repo    language  \\\n",
       "0                            rdpeng/ExData_Plotting1        None   \n",
       "1                     rdpeng/RepData_PeerAssessment1        None   \n",
       "2                  DataScienceSpecialization/courses        HTML   \n",
       "3                              fighting41love/funNLP      Python   \n",
       "4                                   magento/magento2         PHP   \n",
       "..                                               ...         ...   \n",
       "485                                   rprokap/pset-9  JavaScript   \n",
       "486                                 konzy/mass_clone       Shell   \n",
       "487                               biter777/countries          Go   \n",
       "488  sayantann11/all-classification-templetes-for-ML      Python   \n",
       "489                                  denman2328/Help        None   \n",
       "\n",
       "                                       readme_contents  \n",
       "0    ## Introduction\\n\\nThis assignment uses data f...  \n",
       "1    ## Introduction\\n\\nIt is now possible to colle...  \n",
       "2    \\n### Data Science Specialization\\n\\nThese are...  \n",
       "3    <center>\\n    <img style=\"border-radius: 0.312...  \n",
       "4    \\n<p align=\"center\">\\n<a href=\"https://www.cod...  \n",
       "..                                                 ...  \n",
       "485  # pset-9\\n      CREDITS SEQUENCE              ...  \n",
       "486  # mass_clone\\nThis is a shell script that will...  \n",
       "487  # countries\\r\\n\\r\\nCountries - ISO 3166 (ISO31...  \n",
       "488  # all-classification-templetes-for-ML\\nClassif...  \n",
       "489  ------------------\\nSystem Information\\n------...  \n",
       "\n",
       "[490 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('data.json')\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
