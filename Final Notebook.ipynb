{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0482955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import machine learning libaries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import tree\n",
    "\n",
    "# import NLP tool\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# import self-created functions\n",
    "import prepare\n",
    "import project_acquire\n",
    "import explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30791427",
   "metadata": {},
   "source": [
    "# Wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78e9ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use prepare module wrangle data function to acquire data\n",
    "data=pd.read_json('data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f2d68a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<center>\\n    <img style=\"border-radius: 0.3125em;\\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \\n    src=\"./data/.logo图片/.img.jpg\"width=\"180\">\\n    <br>\\n    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\\n    display: inline-block;\\n    color: #999;\\n    padding: 2px;\">NLP民工的乐园</div>\\n</center>\\n<br><br><br><br>\\n\\n[![](https://img.shields.io/github/stars/fighting41love/funnlp?style=social)](https://github.com/fighting41love/funnlp)\\n[![](https://img.shields.io/badge/dynamic/json?color=blue&label=%E7%9F%A5%E4%B9%8E%E5%85%B3%E6%B3%A8&query=%24.data.totalSubs&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dzhihu%26queryKey%3Dmountain-blue-64)](https://www.zhihu.com/people/mountain-blue-64)\\n[![](data/.logo图片/.捐赠图片/.Citations-487-red.svg)](https://scholar.google.com/citations?hl=en&user=aqZdfDUAAAAJ)\\n\\n[![](data/.logo图片/.捐赠图片/.Home-%E4%BA%BA%E7%94%9F%E6%B5%AA%E8%B4%B9%E6%8C%87%E5%8D%97-brightgreen.svg)](https://scholar.google.com/citations?hl=en&user=aqZdfDUAAAAJ)\\n[![](data/.logo图片/.捐赠图片/.%E7%8C%8E%E9%80%81%E9%97%A8-CV-orange.svg)](http://fighting41love.github.io/)\\n<!-- [![](https://img.shields.io/badge/dynamic/json?color=blueviolet&label=github%20followers&query=%24.data.totalSubs&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dgithub%26queryKey%3Dfighting41love)](https://github.com/fighting41love) -->\\n<!-- [![](https://img.shields.io/badge/Homepage-%E4%BA%BA%E7%94%9F%E6%B5%AA%E8%B4%B9%E6%8C%87%E5%8D%97-brightgreen)](http://fighting41love.github.io/archives/) -->\\n\\n### The Most Powerful NLP-Weapon Arsenal\\n\\n## NLP民工的乐园: 几乎最全的中文NLP资源库\\n在入门到熟悉NLP的过程中，用到了很多github上的包，遂整理了一下，分享在这里。\\n\\n很多包非常有趣，值得收藏，满足大家的收集癖！\\n如果觉得有用，请分享并star:star:，谢谢！\\n\\n长期不定时更新，欢迎watch和fork！:heart::heart::heart:\\n\\n \\n|  :eggplant: :cherries: :pear: :tangerine:   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |  :sunflower: :strawberry:  :melon: :tomato: :pineapple: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|\\n|  ----  | ----  |\\n| * [语料库](#语料库)  <br> * [词库及词法工具](#词库及词法工具)  <br> * [预训练语言模型](#预训练语言模型)  <br>  * [抽取](#抽取)  <br> * [知识图谱](#知识图谱)  <br>   * [文本生成](#文本生成) <br>   * [文本摘要](#文本摘要)  <br>  * [智能问答](#智能问答) <br>  * [文本纠错](#文本纠错)  | * [文档处理](#文档处理) <br>   * [表格处理](#表格处理) <br>   * [文本匹配](#文本匹配)  <br>   * [文本数据增强](#文本数据增强) <br>   * [文本检索](#文本检索) <br>  * [阅读理解](#阅读理解) <br>  * [情感分析](#情感分析) <br>  * [常用正则表达式](#常用正则表达式) <br>   * [语音处理](#语音处理) |\\n| * [常用正则表达式](#常用正则表达式) <br>  * [事件抽取](#事件抽取) <br> * [机器翻译](#机器翻译) <br> * [数字转换](#数字转换) <br>  * [指代消解](#指代消解) <br>  * [文本聚类](#文本聚类) <br>  * [文本分类](#文本分类) <br> * [知识推理](#知识推理) <br> * [可解释NLP](#可解释自然语言处理) <br> * [文本对抗攻击](#文本对抗攻击)  |  * [文本可视化](#文本可视化)  <br>  * [文本标注工具](#文本标注工具) <br>  * [综合工具](#综合工具) <br> * [有趣搞笑工具](#有趣搞笑工具) <br> * [课程报告面试等](#课程报告面试等) <br> * [比赛](#比赛) <br> * [金融NLP](#金融自然语言处理) <br> * [医疗NLP](#医疗自然语言处理) <br> * [法律NLP](#法律自然语言处理) <br> * [其他](#其他)  |\\n\\n<!-- \\n目录（Table of contents）\\n=================\\n<table border=\"0\">\\n <tr>\\n    <td><b style=\"font-size:30px\">:star:</b></td>\\n    <td><b style=\"font-size:30px\">:star::star:</b></td>\\n    <td><b style=\"font-size:30px\">:star::star::star:</b></td>\\n    <td><b style=\"font-size:30px\">:star::star::star::star:</b></td>\\n </tr>\\n <tr>\\n    <td>\\n\\n<!--ts-->\\n   <!-- * [语料库](#语料库)\\n   * [词库及词法工具](#词库及词法工具)\\n   * [预训练语言模型](#预训练语言模型)\\n   * [抽取](#抽取)\\n   * [知识图谱](#知识图谱)\\n   * [文本生成](#文本生成)\\n   * [文本摘要](#文本摘要)\\n   * [智能问答](#智能问答)\\n   * [文本纠错](#文本纠错) -->\\n\\n\\n<!--te-->\\n\\n  </td>\\n\\n  <td>\\n\\n<!--ts-->\\n\\n   <!-- * [文档处理](#文档处理)\\n   * [表格处理](#表格处理)\\n   * [文本匹配](#文本匹配)\\n   * [文本数据增强](#文本数据增强)\\n   * [文本检索](#文本检索)\\n   * [阅读理解](#阅读理解)\\n   * [情感分析](#情感分析)\\n   * [常用正则表达式](#常用正则表达式)\\n   * [语音处理](#语音处理) -->\\n<!--te-->\\n\\n  </td>\\n\\n  <td>\\n   \\n<!--ts-->\\n   <!-- * [常用正则表达式](#常用正则表达式)\\n   * [事件抽取](#事件抽取)\\n   * [机器翻译](#机器翻译)\\n   * [数字转换](#数字转换)\\n   * [指代消解](#指代消解)\\n   * [文本聚类](#文本聚类)\\n   * [文本分类](#文本分类)\\n   * [知识推理](#知识推理)\\n   * [可解释NLP](#可解释自然语言处理)\\n   * [文本对抗攻击](#文本对抗攻击) -->\\n\\n<!--te-->\\n    \\n  </td>\\n\\n  <td>\\n   \\n<!--ts-->\\n<!-- \\n   * [文本可视化](#文本可视化)\\n   * [文本标注工具](#文本标注工具)\\n   * [综合工具](#综合工具)\\n   * [有趣搞笑工具](#有趣搞笑工具)\\n   * [课程报告面试等](#课程报告面试等)\\n   * [比赛](#比赛)\\n   * [金融NLP](#金融自然语言处理)\\n   * [医疗NLP](#医疗自然语言处理)\\n   * [法律NLP](#法律自然语言处理)\\n   * [其他](#其他) -->\\n\\n<!--te-->\\n    \\n  <!-- </td>\\n\\n </tr>\\n</table> --> \\n\\n\\n\\n\\n----\\n\\n# 语料库\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :----   |          :--- |\\n|   人名语料库    |        |  [wainshine/Chinese-Names-Corpus](https://github.com/wainshine/Chinese-Names-Corpus)  |\\n|   Chinese-Word-Vectors    |  各种中文词向量      |  [github repo](https://github.com/Embedding/Chinese-Word-Vectors)  |\\n|    中文聊天语料    |   该库搜集了包含豆瓣多轮, PTT八卦语料, 青云语料, 电视剧对白语料, 贴吧论坛回帖语料,微博语料,小黄鸡语料     |  [link](https://github.com/codemayq/chaotbot_corpus_Chinese)  |\\n|    中文谣言数据    |     该数据文件中，每一行为一条json格式的谣言数据   |   [github](https://github.com/thunlp/Chinese_Rumor_Dataset)  |\\n|     中文问答数据集   |        |  [链接](https://panbaiducom/s/1QUsKcFWZ7Tg1dk_AbldZ1A) 提取码 2dva  |\\n|    微信公众号语料   |   3G语料，包含部分网络抓取的微信公众号的文章，已经去除HTML，只包含了纯文本。每行一篇，是JSON格式，name是微信公众号名字，account是微信公众号ID，title是题目，content是正文     | [github](https://github.com/nonamestreet/weixin_public_corpus)    |\\n|    中文自然语言处理 语料、数据集   |        |  [github](https://github.com/SophonPlus/ChineseNlpCorpus)  |\\n|    任务型对话英文数据集    |     【最全任务型对话数据集】主要介绍了一份任务型对话数据集大全，这份数据集大全涵盖了到目前在任务型对话领域的所有常用数据集的主要信息。此外，为了帮助研究者更好的把握领域进展的脉络，我们以Leaderboard的形式给出了几个数据集上的State-of-the-art实验结果。   |   [github](https://github.com/AtmaHou/Task-Oriented-Dialogue-Dataset-Survey)     |\\n|    语音识别语料生成工具    |    从具有音频/字幕的在线视频创建自动语音识别(ASR)语料库    |   [github](https://github.com/yc9701/pansori)  |\\n|     LitBankNLP数据集   |   支持自然语言处理和计算人文学科任务的100部带标记英文小说语料     |   [github](https://github.com/dbamman/litbank) |\\n|    中文ULMFiT  |    情感分析 文本分类 语料及模型    |   [github](https://github.com/bigboNed3/chinese_ulmfit)  |\\n|    省市区镇行政区划数据带拼音标注    |        |   [github](https://github.com/xiangyuecn/AreaCity-JsSpider-StatsGov)  |\\n|    教育行业新闻 自动文摘 语料库    |        |   [github](https://github.com/wonderfulsuccess/chinese_abstractive_corpus)  |\\n|    中文自然语言处理数据集    |        |  [github](https://github.com/InsaneLife/ChineseNLPCorpus)   |\\n|     百度知道问答语料库   |   超过580万的问题，938万的答案，5800个分类标签。基于该问答语料库，可支持多种应用，如闲聊问答，逻辑挖掘     | [github](https://github.com/liuhuanyong/MiningZhiDaoQACorpus)  |\\n|     维基大规模平行文本语料   |  85种语言、1620种语言对、135M对照句  |  [github](https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix) \\n|   古诗词库     |        |  [github repo](https://github.com/panhaiqi/AncientPoetry) <br>[更全的古诗词库](https://github.com/chinese-poetry/chinese-poetry)\\n|   低内存加载维基百科数据    |     用新版nlp库加载17GB+英文维基语料只占用9MB内存遍历速度2-3 Gbit/s    |   [github](https://gistgithub.com/thomwolf/13ca2b2b172b2d17ac66685aa2eeba62) |\\n|    对联数据    |   700,000 couplets, 超过70万对对联     |   [github](https://github.com/wb14123/couplet-dataset)  |\\n|   《配色辞典》数据集     |        |  [github](https://github.com/mattdesl/dictionary-of-colour-combinations)   |\\n|    42GB的JD客服对话数据(CSDD)    |        |   [github](https://github.com/jd-aig/nlp_baai/tree/master/pretrained_models_and_embeddings)  |\\n|  70万对联数据       |        | [link](https://github.com/wb14123/couplet-dataset)   |\\n|   用户名黑名单列表    |        |   [github](https://github.com/marteinn/The-Big-Username-Blacklist)  |\\n|     依存句法分析语料   |    4万句高质量标注数据    |  [Homepage](http//hlt.suda.edu.cn/indexphp/Nlpcc-2019-shared-task)  |\\n|      人民日报语料处理工具集  |        |  [github](https://github.com/howl-anderson/tools_for_corpus_of_people_daily)   |\\n|  虚假新闻数据集 fake news corpus      |        |   [github](https://github.com/several27/FakeNewsCorpus)  |\\n|    诗歌质量评价/细粒度情感诗歌语料库    |        |  [github](https://github.com/THUNLP-AIPoet/Datasets)   |\\n|    中文自然语言处理相关的开放任务    |  数据集以及当前最佳结果     |    [github](https://github.com/didi/ChineseNLP) |\\n|    中文缩写数据集    |        |   [github](https://github.com/zhangyics/Chinese-abbreviation-dataset)  |\\n|    中文任务基准测评     |    代表性的数据集-基准(预训练)模型-语料库-baseline-工具包-排行榜    |   [github](https://github.com/CLUEbenchmark/CLUE)  |\\n|   中文谣言数据库    |        |  [github](https://github.com/thunlp/Chinese_Rumor_Dataset)   |\\n|     CLUEDatasetSearch    |   中英文NLP数据集搜索所有中文NLP数据集，附常用英文NLP数据集     |   [github](https://github.com/CLUEbenchmark/CLUEDatasetSearch)  |\\n|    多文档摘要数据集    |        |  [github](https://github.com/complementizer/wcep-mds-dataset)   |\\n|    让人人都变得“彬彬有礼”礼貌迁移任务   |  在保留意义的同时将非礼貌语句转换为礼貌语句，提供包含139M + 实例的数据集       |   [paper and code](https://arxiv.org/abs/200414257)  |\\n|    粤语/英语会话双语语料库    |        |   [github](https://github.com/khiajohnson/SpiCE-Corpus)  |\\n|     中文NLP数据集列表   |        |   [github](https://github.com/OYE93/Chinese-NLP-Corpus)  |\\n|   类人名/地名/组织机构名的命名体识别数据集     |        |  [github](https://github.com/LG-1/video_music_book_datasets)  |\\n|    中文语言理解测评基准    |    包括代表性的数据集&基准模型&语料库&排行榜   |   [github](https://github.com/brightmart/ChineseGLUE)  |\\n|    OpenCLaP多领域开源中文预训练语言模型仓库    |   民事文书、刑事文书、百度百科 |    [github](https://github.com/thunlp/OpenCLaP) |\\n|   中文全词覆盖BERT及两份阅读理解数据     |      DRCD数据集：由中国台湾台达研究院发布，其形式与SQuAD相同，是基于繁体中文的抽取式阅读理解数据集。<br>CMRC 2018数据集:哈工大讯飞联合实验室发布的中文机器阅读理解数据。根据给定问题，系统需要从篇章中抽取出片段作为答案，形式与SQuAD相同。|    [github](https://github.com/ymcui/Chinese-BERT-wwm) |\\n|  Dakshina数据集     |    十二种南亚语言的拉丁/本地文字平行数据集合     |   [github](https://github.com/google-research-datasets/dakshina)  |\\n|    OPUS-100    |   以英文为中心的多语(100种)平行语料     |   [github](https://github.com/EdinburghNLP/opus-100-corpus)  |\\n|      中文阅读理解数据集  |        |   [github](https://github.com/ymcui/Chinese-RC-Datasets)  |\\n|    中文自然语言处理向量合集    |        |   [github](https://github.com/liuhuanyong/ChineseEmbedding)  |\\n|    中文语言理解测评基准    |包括代表性的数据集、基准(预训练)模型、语料库、排行榜       |  [github](https://github.com/CLUEbenchmark/CLUE)   |\\n|  NLP数据集/基准任务大列表     |        |  [github](https://quantumstatcom/dataset/datasethtml)   |\\n|   LitBankNLP数据集     |   支持自然语言处理和计算人文学科任务的100部带标记英文小说语料     | [github](https://github.com/dbamman/litbank)    |\\n|70万对联数据||[github](https://github.com/wb14123/couplet-dataset)|\\n|文言文（古文）-现代文平行语料|短篇章中包括了《论语》、《孟子》、《左传》等篇幅较短的古籍，已和《资治通鉴》合并|[github](https://github.com/NiuTrans/Classical-Modern)|\\n|COLDDateset，中文冒犯性语言检测数据集|涵盖了种族、性别和地区等话题内容，数据待论文发表后放出|[paper](https://arxiv.org/pdf/2201.06025.pdf)|\\n\\n# 词库及词法工具\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|  textfilter     |    中英文敏感词过滤    |  [observerss/textfilter](https://github.com/observerss/textfilter)  |\\n|   人名抽取功能    |   中文（现代、古代）名字、日文名字、中文的姓和名、称呼（大姨妈、小姨妈等）、英文->中文名字（李约翰）、成语词典   |  [cocoNLP](https://github.com/fighting41love/cocoNLP)  |\\n|   中文缩写库    | 全国人大: 全国 人民 代表大会; 中国: 中华人民共和国;女网赛: 女子/n 网球/n 比赛/vn  |  [github](https://github.com/zhangyics/Chinese-abbreviation-dataset/blob/master/dev_set.txt)  |\\n|   汉语拆字词典    |  漢字\\t拆法 (一)\\t拆法 (二)\\t拆法 (三) 拆\\t手 斥\\t扌 斥\\t才 斥    |  [kfcd/chaizi](https://github.com/kfcd/chaizi)  |\\n|    词汇情感值   |    山泉水:0.400704566541 <br>  充沛:\\t0.37006739587   |   [rainarch/SentiBridge](https://github.com/rainarch/SentiBridge/blob/master/Entity_Emotion_Express/CCF_data/pair_mine_result) |\\n|   中文词库、停用词、敏感词    |        |  [dongxiexidian/Chinese](https://github.com/fighting41love/Chinese_from_dongxiexidian)  |\\n|   python-pinyin    |   汉字转拼音     |  [mozillazg/python-pinyin](https://github.com/mozillazg/python-pinyin)  |\\n|   zhtools   |   中文繁简体互转     |  [skydark/nstools](https://github.com/skydark/nstools/tree/master/zhtools)  |\\n|   英文模拟中文发音引擎    |    say wo i ni #说：我爱你    |  [tinyfool/ChineseWithEnglish](https://github.com/tinyfool/ChineseWithEnglish)  |\\n|  chinese_dictionary     |    同义词库、反义词库、否定词库    |  [guotong1988/chinese_dictionary](https://github.com/guotong1988/chinese_dictionary)  |\\n|   wordninja    |   无空格英文串分割、抽取单词     | [wordninja](https://github.com/keredson/wordninja)  |\\n|   汽车品牌、汽车零件相关词汇    |        |  [data](https://github.com/fighting41love/funNLP/tree/master/data)|     公司名字大全   |        |   [github repo](https://github.com/wainshine/Company-Names-Corpus)\\n|   THU整理的词库   | IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库    | [link](http://thuctc.thunlp.org/)   |\\n|   罪名法务名词及分类模型    |    包含856项罪名知识图谱, 基于280万罪名训练库的罪名预测,基于20W法务问答对的13类问题分类与法律资讯问答功能    |    [github](https://github.com/liuhuanyong/CrimeKgAssitant)     |\\n|   分词语料库+代码    |        |  [百度网盘链接](https://panbaiducom/s/1MXZONaLgeaw0_TxZZDAIYQ)     - 提取码 pea6  |\\n|  基于Bi-LSTM + CRF的中文分词+词性标注     |   keras实现     |  [link](https://github.com/GlassyWing/bi-lstm-crf)  |\\n| 基于Universal Transformer + CRF 的中文分词和词性标注    |        |  [link](https://github.com/GlassyWing/transformer-word-segmenter)  |\\n| 快速神经网络分词包     |    java version     |   [](https://github.com/yaoguangluo/NeroParser) |\\n|   chinese-xinhua      |    中华新华字典数据库及api，包括常用歇后语、成语、词语和汉字    |   [github](https://github.com/pwxcoo/chinese-xinhua)  |\\n|   SpaCy 中文模型     |   包含Parser, NER, 语法树等功能。有一些英文package使用spacy的英文模型的，如果要适配中文，可能需要使用spacy中文模型。     |   [github](https://github.com/howl-anderson/Chinese_models_for_SpaCy)    |\\n|    中文字符数据    |        |  [github](https://github.com/skishore/makemeahanzi)   |\\n|    Synonyms中文近义词工具包    |        |   [github](https://github.com/huyingxi/Synonyms)  |\\n|   HarvestText     |   领域自适应文本挖掘工具（新词发现-情感分析-实体链接等）     |   [github](https://github.com/blmoistawinde/HarvestText)   |\\n|    word2word    |    方便易用的多语言词-词对集62种语言/3,564个多语言对    |   [github](https://github.com/Kyubyong/word2word)  |\\n|   多音字词典数据及代码     |        |  [github](https://github.com/mozillazg/phrase-pinyin-data)   |\\n|    汉字、词语、成语查询接口    |        |   [github](https://github.com/netnr/zidian/tree/206028e5ce9a608afc583820df8dc2d1d4b61781)  |\\n|    103976个英语单词库包    |    （sql版，csv版，Excel版）    |  [github](https://github.com/1eez/103976)   |\\n|    英文脏话大列表    |        |   [github](https://github.com/zacanger/profane-words)  |\\n|      词语拼音数据  |        |   [github](https://github.com/mozillazg/phrase-pinyin-data)  |\\n|   186种语言的数字叫法库     |        |   [github](https://github.com/google/UniNum)  |\\n|    世界各国大规模人名库    |        |   [github](https://github.com/philipperemy/name-dataset)  |\\n|   汉字字符特征提取器 (featurizer)     |   提取汉字的特征（发音特征、字形特征）用做深度学习的特征     |   [github](https://github.com/howl-anderson/hanzi_char_featurizer)  |\\n|     char_featurizer - 汉字字符特征提取工具   |        |    [github](https://github.com/charlesXu86/char_featurizer) |\\n|   中日韩分词库mecab的Python接口库     |        |   [github](https://github.com/jeongukjae/python-mecab)  |\\n|    g2pC基于上下文的汉语读音自动标记模块    |        |   [github](https://github.com/Kyubyong/g2pC)  |\\n|     ssc, Sound Shape Code   | 音形码 - 基于“音形码”的中文字符串相似度计算方法      | [version 1](https://github.com/qingyujean/ssc)<br>[version 2](https://github.com/wenyangchou/SimilarCharactor)<br>[blog/introduction](https://blogcsdnnet/chndata/article/details/41114771)   |\\n|    基于百科知识库的中文词语多词义/义项获取与特定句子词语语义消歧    |        |    [github](https://github.com/liuhuanyong/WordMultiSenseDisambiguation) |\\n|   Tokenizer快速、可定制的文本词条化库     |        |   [github](https://github.com/OpenNMT/Tokenizer)  |\\n|   Tokenizers     |  注重性能与多功能性的最先进分词器      |    [github](https://github.com/huggingface/tokenizers)|\\n|    通过同义词替换实现文本“变脸”    |        |    [github](https://github.com/paubric/python-sirajnet) |\\n|    token2index与PyTorch/Tensorflow兼容的强大轻量词条索引库    |        |  [github](https://github.com/Kaleidophon/token2index)   |\\n|    繁简体转换    |        |   [github](https://github.com/berniey/hanziconv)  |\\n| 粤语NLP工具|       |   [github](https://github.com/jacksonllee/pycantonese)|\\n|领域词典库|涵盖68个领域、共计916万词的专业词典知识库|[github](github.com/liuhuanyong/DomainWordsDict)|\\n\\n\\n\\n# 预训练语言模型&大模型\\n   \\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|BMList|大模型大列表|[github](https://github.com/OpenBMB/BMList)|\\n| bert论文中文翻译     |        | [link](https://github.com/yuanxiaosc/BERT_Paper_Chinese_Translation)   |\\n|    bert原作者的slides  |    |  [link](https://pan.baidu.com/s/1OSPsIu2oh1iJ-bcXoDZpJQ)  |\\n| 文本分类实践     |        |  [github](https://github.com/NLPScott/bert-Chinese-classification-task)  |\\n|  bert tutorial文本分类教程     |        | [github](https://github.com/Socialbird-AILab/BERT-Classification-Tutorial) |\\n| bert pytorch实现       |        |  [github](https://github.com/huggingface/pytorch-pretrained-BERT)  |\\n|   bert pytorch实现      |        |  [github](https://github.com/huggingface/pytorch-pretrained-BERT)  |\\n|  BERT生成句向量，BERT做文本分类、文本相似度计算     |        | [github](https://github.com/terrifyzhao/bert-utils)   |\\n|  bert、ELMO的图解     |        |  [github](https://jalammargithubio/illustrated-bert/)  |\\n|  BERT Pre-trained models and downstream applications     |        |  [github](https://github.com/asyml/texar/tree/master/examples/bert)  |\\n|  语言/知识表示工具BERT & ERNIE      |        |   [github](https://github.com/PaddlePaddle/LARK)  |\\n|    Kashgari中使用gpt-2语言模型    |        |  [github](https://github.com/BrikerMan/Kashgari)   |\\n|     Facebook LAMA   |    用于分析预训练语言模型中包含的事实和常识知识的探针。语言模型分析，提供Transformer-XL/BERT/ELMo/GPT预训练语言模型的统一访问接口    |  [github](https://github.com/facebookresearch/LAMA)   |\\n|    中文的GPT2训练代码    |        |    [github](https://github.com/Morizeyao/GPT2-Chinese) |\\n|   XLMFacebook的跨语言预训练语言模型     |        |   [github](https://github.com/facebookresearch/XLM)  |\\n|    海量中文预训练ALBERT模型    |        |  [github](https://github.com/brightmart/albert_zh)   |\\n|    Transformers 20    |    支持TensorFlow 20 和 PyTorch 的自然语言处理预训练语言模型(BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) 8种架构/33种预训练模型/102种语言   | [github](https://github.com/huggingface/transformers)    |\\n|    8篇论文梳理BERT相关模型进展与反思    |        |    [github](https://wwwmsracn/zh-cn/news/features/bert) |\\n|    法文RoBERTa预训练语言模型    |    用138GB语料训练的法文RoBERTa预训练语言模型    |   [link](https://camembert-model.fr/)  |\\n|     中文预训练 ELECTREA 模型    |    基于对抗学习 pretrain Chinese Model    |   [github](https://github.com/CLUEbenchmark/ELECTRA)  |\\n|   albert-chinese-ner     |   用预训练语言模型ALBERT做中文NER    |   [github](https://github.com/ProHiryu/albert-chinese-ner)  |\\n|    开源预训练语言模型合集    |        |  [github](https://github.com/ZhuiyiTechnology/pretrained-models)   |\\n|   中文ELECTRA预训练模型     |        |  [github](https://github.com/ymcui/Chinese-ELECTRA)   |\\n|    用Transformers(BERT, XLNet, Bart, Electra, Roberta, XLM-Roberta)预测下一个词(模型比较)    |        |  [github](https://github.com/renatoviolin/next_word_prediction)   |\\n|   TensorFlow Hub     |    40+种语言的新语言模型(包括中文)    |  [link](https://tfhub.dev/google/collections/wiki40b-lm/1)   |\\n|   UER     | 基于不同语料、编码器、目标任务的中文预训练模型仓库（包括BERT、GPT、ELMO等）       |  [github](https://github.com/dbiir/UER-py)    |\\n|    开源预训练语言模型合集    |        |  [github](https://github.com/ZhuiyiTechnology/pretrained-models)   |\\n|   多语言句向量包     |        |  [github](https://github.com/yannvgn/laserembeddings)   |\\n|Language Model as a Service (LMaaS)|语言模型即服务|[github](https://github.com/txsun1997/LMaaS-Papers)|\\n|开源语言模型GPT-NeoX-20B|200亿参数，是目前最大的可公开访问的预训练通用自回归语言模型|[github](https://github.com/EleutherAI/gpt-neox)|\\n|中文科学文献数据集（CSL）|包含 396,209 篇中文核心期刊论文元信息 （标题、摘要、关键词、学科、门类）。CSL 数据集可以作为预训练语料，也可以构建许多NLP任务，例如文本摘要（标题预测）、 关键词生成和文本分类等。|[github](https://github.com/ydli-ai/CSL)|\\n|大模型开发神器||[github](https://github.com/hpcaitech/ColossalAI)|\\n\\n# 抽取\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   时间抽取   |    已集成到 python package [cocoNLP](https://github.com/fighting41love/cocoNLP)中，欢迎试用    | [java version]( https://github.com/shinyke/Time-NLP)<br>[python version](https://github.com/zhanzecheng/Time_NLP) |\\n|    神经网络关系抽取 pytorch    |    暂不支持中文     |    [github](https://github.com/ShulinCao/OpenNRE-PyTorch)    |\\n|    基于bert的命名实体识别 pytorch    |  暂不支持中文      |   [github](https://github.com/Kyubyong/bert_ner)     |\\n|   关键词(Keyphrase)抽取包 pke     |        |   [github](https://github.com/boudinfl/pke)     |\\n|    BLINK最先进的实体链接库    |        |   [github](https://github.com/facebookresearch/BLINK)  |\\n|   BERT/CRF实现的命名实体识别     |        |   [github](https://github.com/Louis-udm/NER-BERT-CRF)  |\\n|    支持批并行的LatticeLSTM中文命名实体识别    |        |  [github](https://github.com/LeeSureman/Batch_Parallel_LatticeLSTM)   |\\n|    构建医疗实体识别的模型 |   包含词典和语料标注，基于python    |  [github](https://github.com/yixiu00001/LSTM-CRF-medical)   |\\n|    基于TensorFlow和BERT的管道式实体及关系抽取    |       - Entity and Relation Extraction Based on TensorFlow and BERT 基于TensorFlow和BERT的管道式实体及关系抽取，2019语言与智能技术竞赛信息抽取任务解决方案。Schema based Knowledge Extraction, SKE 2019   |   [github](https://github.com/yuanxiaosc/Entity-Relation-Extraction)  |\\n| 中文命名实体识别NeuroNER vs BertNER       |        |    [github](https://github.com/EOA-AILab/NER-Chinese) |\\n|  基于BERT的中文命名实体识别      |        |  [github](https://github.com/lonePatient/BERT-NER-Pytorch)   |\\n|    中文关键短语抽取工具    |        |  [github](https://github.com/dongrixinyu/chinese_keyphrase_extractor)   |\\n| bert      |     用于中文命名实体识别 tensorflow版本   |   [github](https://github.com/macanv/BERT-BiLSTM-CRF-NER)  |\\n|   bert-Kashgari     |    基于 keras 的封装分类标注框架 Kashgari，几分钟即可搭建一个分类或者序列标注模型     |  [github](https://github.com/BrikerMan/Kashgari)  |\\n|    cocoNLP    |  人名、地址、邮箱、手机号、手机归属地 等信息的抽取，rake短语抽取算法。  |   [github](https://github.com/fighting41love/cocoNLP)|\\n|    Microsoft多语言数字/单位/如日期时间识别包    |        |  [github](https://github.com/Microsoft/Recognizers-Text)   |\\n| 百度开源的基准信息抽取系统       |        |   [github](https://github.com/baidu/information-extraction)  |\\n|    中文地址分词（地址元素识别与抽取），通过序列标注进行NER    |        |  [github](https://github.com/yihenglu/chinese-address-segment)   |\\n|    基于依存句法的开放域文本知识三元组抽取和知识库构建    |        |   [github](https://github.com/lemonhu/open-entity-relation-extraction)  |\\n|   基于预训练模型的中文关键词抽取方法     |        |   [github](https://github.com/sunyilgdx/SIFRank_zh)  |\\n|  chinese_keyphrase_extractor (CKPE)      |  A tool for chinese keyphrase extraction 一个快速从自然语言文本中提取和识别关键短语的工具    |   [github](https://github.com/dongrixinyu/chinese_keyphrase_extractor)  |\\n|    简单的简历解析器，用来从简历中提取关键信息    |        |   [github](https://github.com/OmkarPathak/pyresparser)  |\\n|   BERT-NER-Pytorch三种不同模式的BERT中文NER实验    |        |  [github](https://github.com/lonePatient/BERT-NER-Pytorch)   |\\n\\n\\n\\n\\n# 知识图谱\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|    清华大学XLORE中英文跨语言百科知识图谱    |   百度、中文维基、英文维基    |  [link](https://xlore.org/downloadhtml)     |\\n|    文档图谱自动生成    |        |   [github](https://github.com/liuhuanyong/TextGrapher)    |\\n|     基于医疗领域知识图谱的问答系统   |        |   [github](https://github.com/zhihao-chen/QASystemOnMedicalGraph) <br>该repo参考了[github](https://github.com/liuhuanyong/QASystemOnMedicalKG)   |\\n|    中文人物关系知识图谱项目    |        |   [github](https://github.com/liuhuanyong/PersonRelationKnowledgeGraph)  |\\n|    AmpliGraph 知识图谱表示学习(Python)库知识图谱概念链接预测    |        |  [github](https://github.com/Accenture/AmpliGraph)    |\\n|    中文知识图谱资料、数据及工具    |        |    [github](https://github.com/husthuke/awesome-knowledge-graph) |\\n|    基于百度百科的中文知识图谱  |     抽取三元组信息，构建中文知识图谱     |   [github](https://github.com/lixiang0/WEB_KG)  |\\n|    Zincbase 知识图谱构建工具包    |        |  [github](https://github.com/tomgrek/zincbase)   |\\n|    基于知识图谱的问答系统    |        |  [github](https://github.com/WenRichard/KBQA-BERT)   |\\n|    知识图谱深度学习相关资料整理    |        |   [github](https://github.com/lihanghang/Knowledge-Graph)  |\\n|   东南大学《知识图谱》研究生课程(资料)     |        |  [github](https://github.com/npubird/KnowledgeGraphCourse)   |\\n|    知识图谱车音工作项目    |        |   [github](https://github.com/qiu997018209/KnowledgeGraph)  |\\n|     《海贼王》知识图谱   |        |   [github](https://github.com/mrbulb/ONEPIECE-KG)  |\\n|    132个知识图谱的数据集    |    涵盖常识、城市、金融、农业、地理、气象、社交、物联网、医疗、娱乐、生活、商业、出行、科教    |   [link](http//openkg.cn)  |\\n|    大规模、结构化、中英文双语的新冠知识图谱(COKG-19)    |        |   [link](http://www.openkg.cn/dataset?q=COKG-19)  |\\n|    基于依存句法与语义角色标注的事件三元组抽取    |        |    [github](https://github.com/liuhuanyong/EventTriplesExtraction)   |\\n|     抽象知识图谱  |   目前规模50万，支持名词性实体、状态性描述、事件性动作进行抽象      |   [github](https://github.com/liuhuanyong/AbstractKnowledgeGraph)  |\\n|    大规模中文知识图谱数据14亿实体    |        |   [github](https://github.com/ownthink/KnowledgeGraphData)  |\\n|    Jiagu自然语言处理工具     |    以BiLSTM等模型为基础，提供知识图谱关系抽取 中文分词 词性标注 命名实体识别 情感分析 新词发现 关键词 文本摘要 文本聚类等功能    |  [github](https://github.com/ownthink/Jiagu)   |\\n|     medical_NER - 中文医学知识图谱命名实体识别   |        |    [github](https://github.com/pumpkinduo/KnowledgeGraph_NER) |\\n|   知识图谱相关学习资料/数据集/工具资源大列表     |        |  [github](https://github.com/totogo/awesome-knowledge-graph)   |\\n|    LibKGE面向可复现研究的知识图谱嵌入库    |        |  [github](https://github.com/uma-pi1/kge)   |\\n|   基于mongodb存储的军事领域知识图谱问答项目    |    包括飞行器、太空装备等8大类，100余小类，共计5800项的军事武器知识库，该项目不使用图数据库进行存储，通过jieba进行问句解析，问句实体项识别，基于查询模板完成多类问题的查询，主要是提供一种工业界的问答思想demo。    |   [github](https://github.com/liuhuanyong/QAonMilitaryKG)  |\\n|     京东商品知识图谱   |        |   [github](https://github.com/liuhuanyong/ProductKnowledgeGraph)  |\\n|    基于远监督的中文关系抽取    |        |    [github](https://github.com/xiaolalala/Distant-Supervised-Chinese-Relation-Extraction) |\\n|  基于医药知识图谱的智能问答系统      |        |   [github](https://github.com/YeYzheng/KGQA-Based-On-medicine) |\\n|    BLINK最先进的实体链接库    |        |   [github](https://github.com/facebookresearch/BLINK)  |\\n|   一个小型的证券知识图谱/知识库     |        |    [github](https://github.com/lemonhu/stock-knowledge-graph) |\\n|   dstlr非结构化文本可扩展知识图谱构建平台     |        |   [github](https://github.com/dstlry/dstlr)  |\\n|  百度百科人物词条属性抽取    |  用基于BERT的微调和特征提取方法来进行知识图谱      |   [github](https://github.com/sakuranew/BERT-AttributeExtraction)|\\n|   新冠肺炎相关数据     |  新冠及其他类型肺炎中文医疗对话数据集；清华大学等机构的开放数据源（COVID-19）   | [github](https://www.aminer.cn/data-covid19/)<br>  [github](https://github.com/UCSD-AI4H/COVID-Dialogue) |\\n|   DGL-KE 图嵌入表示学习算法     |        |   [github](https://github.com/awslabs/dgl-ke)  |\\n|因果关系图谱||[method](https://github.com/liuhuanyong/CausalityEventExtraction) [data](https://github.com/fighting41love/CausalDataset)|\\n|基于多领域文本数据集的因果事件对||[link](http://thuctc.thunlp.org/)|\\n\\n# 文本生成\\n \\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   Texar    |   Toolkit for Text Generation and Beyond     |  [github](https://github.com/asyml/texar)  |\\n|   Ehud Reiter教授的博客    |        | [link](https://ehudreiter.com)  北大万小军教授强力推荐，该博客对NLG技术、评价与应用进行了深入的探讨与反思。   |\\n|   文本生成相关资源大列表    |        | [github](https://github.com/ChenChengKuan/awesome-text-generation)     |\\n|  开放域对话生成及在微软小冰中的实践      |   自然语言生成让机器掌握自动创作的本领    |   [link](https://drive.google.com/file/d/1Mdna3q986k6OoJNsfAHznTtnMAEVzv5z/view)  |\\n|    文本生成控制   |        |  [github](https://github.com/harvardnlp/Talk-Latent/blob/master/mainpdf)    |\\n|    自然语言生成相关资源大列表   |        |   [github](https://github.com/tokenmill/awesome-nlg)  |\\n|    用BLEURT评价自然语言生成   |        |  [link](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html)  |\\n|   自动对联数据及机器人    |        | [代码 link](https://github.com/wb14123/seq2seq-couplet) <br> [70万对联数据](https://github.com/wb14123/couplet-dataset)     |\\n|   自动生成评论     |   用Transformer编解码模型实现的根据Hacker News文章标题生成评论     |   [github](https://github.com/leod/hncynic)  |\\n|    自然语言生成SQL语句（英文）    |        |   [github](https://github.com/paulfitz/mlsql)  |\\n|    自然语言生成资源大全    |        |   [github](https://github.com/tokenmill/awesome-nlg)  |\\n|    中文生成任务基准测评    |        |  [github](https://github.com/CLUEbenchmark/CLGE)   |\\n|     基于GPT2的特定主题文本生成/文本增广   |        |   [github](https://github.com/prakhar21/TextAugmentation-GPT2)  |\\n|     编码、标记和实现一种可控高效的文本生成方法   |        |   [github](https://github.com/yannvgn/laserembeddings)  |\\n|    TextFooler针对文本分类/推理的对抗文本生成模块    |        |   [github](https://github.com/jind11/TextFooler)  |\\n|    SimBERT     |基于UniLM思想、融检索与生成于一体的BERT模型        |   [github](https://github.com/ZhuiyiTechnology/simbert)  |\\n|    新词生成及造句    |    不存在的词用GPT-2变体从头生成新词及其定义、例句    |    [github](https://github.com/turtlesoupy/this-word-does-not-exist) |\\n|   由文本自动生成多项选择题     |        |   [github](https://github.com/KristiyanVachev/Question-Generation)  |\\n|     合成数据生成基准   |        |  [github](https://github.com/sdv-dev/SDGym)   |\\n|       |        |    |\\n\\n# 文本摘要\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   中文文本摘要/关键词提取     |        |    [github](https://github.com/letiantian/TextRank4ZH) |\\n|    基于命名实体识别的简历自动摘要    |        |   [github](https://github.com/DataTurks-Engg/Entity-Recognition-In-Resumes-SpaCy)  |\\n|    文本自动摘要库TextTeaser     |  仅支持英文      |   [github](https://github.com/IndigoResearch/textteaser)  |\\n|    基于BERT等最新语言模型的抽取式摘要提取    |        |   [github](https://github.com/Hellisotherpeople/CX_DB8)  |\\n|   Python利用深度学习进行文本摘要的综合指南     |        |   [link](https://mp.weixin.qq.com/s/gDZyTbM1nw3fbEnU--y3nQ)  |\\n|   (Colab)抽象文本摘要实现集锦(教程     |        |   [github](https://github.com/theamrzaki/text_summurization_abstractive_methods)  |\\n\\n\\n# 智能问答\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|    中文聊天机器人    |  根据自己的语料训练出自己想要的聊天机器人，可以用于智能客服、在线问答、智能聊天等场景      |   [github](https://github.com/Doragd/Chinese-Chatbot-PyTorch-Implementation)  |\\n|   有趣的情趣robot qingyun      |    qingyun 训练出来的中文聊天机器人     |   [github](https://github.com/Doragd/Chinese-Chatbot-PyTorch-Implementation)  |\\n|     开放了对话机器人、知识图谱、语义理解、自然语言处理工具及数据   |        |   [github](https://wwwownthinkcom/#header-n30)  |\\n|  qa对的机器人      |    Amodel-for-Retrivalchatbot - 客服机器人，Chinese Retreival chatbot（中文检索式机器人）    | [git](https://github.com/WenRichard/QAmodel-for-Retrievalchatbot)   |\\n|  ConvLab开源多域端到端对话系统平台      |        |  [github](https://github.com/ConvLab/ConvLab)   |\\n|   基于最新版本rasa搭建的对话系统     |        |   [github](https://github.com/GaoQ1/rasa_chatbot_cn)  |\\n|   基于金融-司法领域(兼有闲聊性质)的聊天机器人     |        |   [github](https://github.com/charlesXu86/Chatbot_CN)  |\\n|    端到端的封闭域对话系统    |        |  [github](https://github.com/cdqa-suite/cdQA)   |\\n|     MiningZhiDaoQACorpus    |    580万百度知道问答数据挖掘项目，百度知道问答语料库，包括超过580万的问题，每个问题带有问题标签。基于该问答语料库，可支持多种应用，如逻辑挖掘    |    [github]() |\\n|   用于中文闲聊的GPT2模型GPT2-chitchat     |        |    [github](https://github.com/yangjianxin1/GPT2-chitchat) |\\n|    基于检索聊天机器人多轮响应选择相关资源列表(Leaderboards、Datasets、Papers)    |        |   [github](https://github.com/JasonForJoy/Leaderboards-for-Multi-Turn-Response-Selection)  |\\n|   微软对话机器人框架     |        |    [github](https://github.com/microsoft/botframework) |\\n|      chatbot-list  |   行业内关于智能客服、聊天机器人的应用和架构、算法分享和介绍    |  [github](https://github.com/lizhe2004/chatbot-list)   |\\n|     Chinese medical dialogue data 中文医疗对话数据集   |        |   [github](https://github.com/Toyhom/Chinese-medical-dialogue-data)  |\\n|    一个大规模医疗对话数据集    |   包含110万医学咨询，400万条医患对话    |    [github](https://github.com/UCSD-AI4H/Medical-Dialogue-System) |\\n|    大规模跨领域中文任务导向多轮对话数据集及模型CrossWOZ    |        |  [paper & data](https://arxiv.org/pdf/200211893pdf)   |\\n|   开源对话式信息搜索平台     |        |    [github](https://github.com/microsoft/macaw) |\\n|      情境互动多模态对话挑战2020(DSTC9 2020)  |        |  [github](https://github.com/facebookresearch/simmc)   |\\n|    用Quora问题对训练的T5问题意译(Paraphrase)    |        |   [github](https://github.com/renatoviolin/T5-paraphrase-generation)  |\\n|    Google发布Taskmaster-2自然语言任务对话数据集    |        |   [github](https://github.com/google-research-datasets/Taskmaster/tree/master/TM-2-2020)  |\\n|    Haystack灵活、强大的可扩展问答(QA)框架    |        |   [github](https://github.com/deepset-ai/haystack)  |\\n|    端到端的封闭域对话系统    |        |   [github](https://github.com/cdqa-suite/cdQA)  |\\n|   Amazon发布基于知识的人-人开放领域对话数据集     |        |   [github](https://github.com/alexa/alexa-prize-topical-chat-dataset/)  |\\n|    基于百度webqa与dureader数据集训练的Albert Large QA模型    |        |   [github](https://github.com/wptoux/albert-chinese-large-webqa/tree/master)  |\\n|   CommonsenseQA面向常识的英文QA挑战     |        |   [link](https://www.tau-nlp.org/commonsenseqa)  |\\n|   MedQuAD(英文)医学问答数据集     |        |  [github](https://github.com/abachaa/MedQuAD)   |\\n|    基于Albert、Electra，用维基百科文本作为上下文的问答引擎    |        |   [github](https://github.com/renatoviolin/Question-Answering-Albert-Electra)  |\\n|   基于14W歌曲知识库的问答尝试    |     功能包括歌词接龙，已知歌词找歌曲以及歌曲歌手歌词三角关系的问答   |    [github](https://github.com/liuhuanyong/MusicLyricChatbot) |\\n\\n\\n# 文本纠错\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|  中文文本纠错模块代码      |        |  [github](https://github.com/zedom1/error-detection)   |\\n|    英文拼写检查库    |        |   [github](https://github.com/barrust/pyspellchecker)  |\\n|  python拼写检查库      |        |  [github](https://github.com/barrust/pyspellchecker)   |\\n|    GitHub Typo Corpus大规模GitHub多语言拼写错误/语法错误数据集    |        |   [github](https://github.com/mhagiwara/github-typo-corpus)  |\\n|    BertPunc基于BERT的最先进标点修复模型    |        |   [github](https://github.com/nkrnrnk/BertPunc)  |\\n|    中文写作校对工具    |        |  [github](https://xiezuocat.com/#/)   |\\n|文本纠错文献列表| Chinese Spell Checking (CSC) and Grammatical Error Correction (GEC)|[github](https://github.com/nghuyong/text-correction-papers)|\\n|文本智能校对大赛冠军方案|已落地应用，来自苏州大学、达摩院团队|[link](https://mp.weixin.qq.com/s/2TjpmoYnt2BUTQVLi26AFA)|\\n\\n\\n# 多模态\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|中文多模态数据集「悟空」|华为诺亚方舟实验室开源大型，包含1亿图文对|[github](https://wukong-dataset.github.io/wukong-dataset/)|\\n\\n\\n# 语音处理\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|    ASR 语音数据集 + 基于深度学习的中文语音识别系统    |        |    [github](https://github.com/nl8590687/ASRT_SpeechRecognition)  |\\n|   清华大学THCHS30中文语音数据集    |        |  [data_thchs30tgz-OpenSLR国内镜像](<http//cn-mirroropenslrorg/resources/18/data_thchs30tgz>)<br>[data_thchs30tgz](<http//wwwopenslrorg/resources/18/data_thchs30tgz>) <br>[test-noisetgz-OpenSLR国内镜像](<http//cn-mirroropenslrorg/resources/18/test-noisetgz>)[test-noisetgz](<http//wwwopenslrorg/resources/18/test-noisetgz>) <br>[resourcetgz-OpenSLR国内镜像](<http//cn-mirroropenslrorg/resources/18/resourcetgz>)<br>[resourcetgz](<http//wwwopenslrorg/resources/18/resourcetgz>)<br>[Free ST Chinese Mandarin Corpus](<http//cn-mirroropenslrorg/resources/38/ST-CMDS-20170001_1-OStargz>)<br>[Free ST Chinese Mandarin Corpus](<http//wwwopenslrorg/resources/38/ST-CMDS-20170001_1-OStargz>)<br>[AIShell-1 开源版数据集-OpenSLR国内镜像](<http//cn-mirroropenslrorg/resources/33/data_aishelltgz>)<br>[AIShell-1 开源版数据集](<http//wwwopenslrorg/resources/33/data_aishelltgz>)<br>[Primewords Chinese Corpus Set 1-OpenSLR国内镜像](<http//cn-mirroropenslrorg/resources/47/primewords_md_2018_set1targz>)<br>[Primewords Chinese Corpus Set 1](<http//wwwopenslrorg/resources/47/primewords_md_2018_set1targz>) |\\n|    笑声检测器    |        |    [github](https://github.com/ideo/LaughDetection)  |\\n|    Common Voice语音识别数据集新版    |  包括来自42,000名贡献者超过1,400小时的语音样本，涵github     |   [link](https://voice.mozilla.org/en/datasets)     |\\n|    speech-aligner    |  从“人声语音”及其“语言文本”，产生音素级别时间对齐标注的工具       |   [github](https://github.com/open-speech/speech-aligner)  |\\n|   ASR语音大辞典/词典     |        |   [github](hhttps://github.com/aishell-foundation/DaCiDian)  |\\n|     语音情感分析   |        |   [github](https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer)  |\\n|    masr     | 中文语音识别，提供预训练模型，高识别率       |   [github](https://github.com/lukhy/masr)  |\\n|    面向语音识别的中文文本规范化    |        |  [github](https://github.com/speech-io/chinese_text_normalization)   |\\n|      语音质量评价指标(MOSNet, BSSEval, STOI, PESQ, SRMR)  |        |   [github](https://github.com/aliutkus/speechmetrics)  |\\n|    面向语音识别的中文/英文发音辞典    |        |   [github](https://github.com/speech-io/BigCiDian)  |\\n|    CoVoSTFacebook发布的多语种语音-文本翻译语料库    | 包括11种语言(法语、德语、荷兰语、俄语、西班牙语、意大利语、土耳其语、波斯语、瑞典语、蒙古语和中文)的语音、文字转录及英文译文      |    [github](https://github.com/facebookresearch/covost) |\\n|    Parakeet基于PaddlePaddle的文本-语音合成    |        |   [github](https://github.com/PaddlePaddle/Parakeet)  |\\n|     (Java)准确的语音自然语言检测库   |        |   [github](https://github.com/pemistahl/lingua)  |\\n|   CoVoSTFacebook发布的多语种语音-文本翻译语料库     |        |   [github](https://github.com/facebookresearch/covost)  |\\n|   TensorFlow 2 实现的文本语音合成     |        |   [github](https://github.com/as-ideas/TransformerTTS)  |\\n|    Python音频特征提取包    |        |  [github](https://github.com/novoic/surfboard)   |\\n|   ViSQOL音频质量感知客观、完整参考指标，分音频、语音两种模式     |        |  [github](https://github.com/google/visqol)   |\\n|    zhrtvc    |     好用的中文语音克隆兼中文语音合成系统     |  [github](https://github.com/KuangDD/zhrtvc)  |\\n|      aukit    |  好用的语音处理工具箱，包含语音降噪、音频格式转换、特征频谱生成等模块       |  [github](https://github.com/KuangDD/aukit)  |\\n|      phkit    |   好用的音素处理工具箱，包含中文音素、英文音素、文本转拼音、文本正则化等模块     |  [github](https://github.com/KuangDD/phkit)   |\\n|     zhvoice     |   中文语音语料，语音更加清晰自然，包含8个开源数据集，3200个说话人，900小时语音，1300万字     |  [github](https://github.com/KuangDD/zhvoice)   |\\n|   audio面向语音行为检测     |  、二值化、说话人识别、自动语音识别、情感识别等任务的音频标注工具      |  [github](https://github.com/midas-research/audino)   |\\n|     深度学习情感文本语音合成   |        |  [github](https://github.com/Emotional-Text-to-Speech/dl-for-emo-tts)   |\\n|   Python音频数据增广库     |        |   [github](https://github.com/iver56/audiomentations)  |\\n|   基于大规模音频数据集Audioset的音频增强     |        |    [github](https://github.com/AppleHolic/audioset_augmentor) |\\n|    语声迁移    |        |   [github](https://github.com/fighting41love/become-yukarin)  |\\n\\n\\n\\n# 文档处理\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   PyLaia面向手写文档分析的深度学习工具包     |        |   [github](https://github.com/jpuigcerver/PyLaia)  |\\n|    单文档非监督的关键词抽取    |        |  [github](https://github.com/LIAAD/yake)   |\\n|      DocSearch免费文档搜索引擎  |        |   [github](https://github.com/algolia/docsearch)  |\\n|  fdfgen       |    能够自动创建pdf文档，并填写信息     | [link](https://github.com/ccnmtl/fdfgen)   |\\n| pdfx       |   自动抽取出引用参考文献，并下载对应的pdf文件 | [link](https://github.com/metachris/pdfx)   |\\n|     invoice2data   |   发票pdf信息抽取     |  [invoice2data](https://github.com/invoice-x/invoice2data)  |\\n|   pdf文档信息抽取    |        |  [github](https://github.com/jstockwin/py-pdf-parser)   |\\n|PDFMiner     |     PDFMiner能获取页面中文本的准确位置，以及字体或行等其他信息。它还有一个PDF转换器，可以将PDF文件转换成其他文本格式(如HTML)。还有一个可扩展的解析器PDF，可以用于文本分析以外的其他用途。   |  [link](https://github.com/euske/pdfminer)  |\\n|  PyPDF2      |    PyPDF 2是一个python PDF库，能够分割、合并、裁剪和转换PDF文件的页面。它还可以向PDF文件中添加自定义数据、查看选项和密码。它可以从PDF检索文本和元数据，还可以将整个文件合并在一起。    |  [link](https://github.com/mstamy2/PyPDF2)   |\\n|   PyPDF2     |     PyPDF 2是一个python PDF库，能够分割、合并、裁剪和转换PDF文件的页面。它还可以向PDF文件中添加自定义数据、查看选项和密码。它可以从PDF检索文本和元数据，还可以将整个文件合并在一起。  |   [link](https://github.com/mstamy2/PyPDF2)  |\\n|    ReportLab   |      ReportLab能快速创建PDF 文档。经过时间证明的、超好用的开源项目，用于创建复杂的、数据驱动的PDF文档和自定义矢量图形。它是免费的，开源的，用Python编写的。该软件包每月下载5万多次，是标准Linux发行版的一部分，嵌入到许多产品中，并被选中为Wikipedia的打印/导出功能提供动力。  | [link](https://www.reportlab.com/opensource/)   |\\n|    SIMPdfPython写的简单PDF文件文字编辑器    |        |   [github](https://github.com/shashanoid/Simpdf)  |\\n|pdf-diff |PDF文件diff工具 可显示两个pdf文档的差别| [github](https://github.com/serhack/pdf-diff)|\\n\\n# 表格处理\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|  用unet实现对文档表格的自动检测，表格重建      |        |   [github](https://github.com/chineseocr/table-ocr)  |\\n|   pdftabextract    |  用于OCR识别后的表格信息解析，很强大      |    [link](https://github.com/WZBSocialScienceCenter/pdftabextract)   |\\n| tabula-py    |     直接将pdf中的表格信息转换为pandas的dataframe，有java和python两种版本代码   |    [](https://github.com/chezou/tabula-py) |\\n|   camelot     |   pdf表格解析       |  [link](https://github.com/atlanhq/camelot)  |\\n| pdfplumber      |   pdf表格解析     | [](https://github.com/jsvine/pdfplumber)   |\\n|   PubLayNet   |     能够划分段落、识别表格、图片   |  [link](https://github.com/ibm-aur-nlp/PubTabNet)  |\\n|    从论文中提取表格数据 |        |    [github](https://github.com/paperswithcode/axcell) |\\n|    用BERT在表格中寻找答案    |        |  [github](https://github.com/google-research/tapas)   |\\n|    表格问答的系列文章    |        |  [简介](https://mp.weixin.qq.com/s?__biz=MzAxMDk0OTI3Ng==&mid=2247484103&idx=2&sn=4a5b50557ab9178270866d812bcfc87f&chksm=9b49c534ac3e4c22de7c53ae5d986fac60a7641c0c072d4038d9d4efd6beb24a22df9f859d08&scene=21#wechat_redirect)<br>[模型](https://mp.weixin.qq.com/s?__biz=MzAxMDk0OTI3Ng==&mid=2247484103&idx=1&sn=73f37fbc1dbd5fdc2d4ad54f58693ef3&chksm=9b49c534ac3e4c222f6a320674b3728cf8567b9a16e6d66b8fdcf06703b05a16a9c9ed9d79a3&scene=21#wechat_redirect)<br>[完结篇](https://mp.weixin.qq.com/s/ee1DG_vO2qblqFC6zO97pA)  |\\n|     使用GAN生成表格数据（仅支持英文）   |        |   [github](https://github.com/Diyago/GAN-for-tabular-data)  |\\n|  carefree-learn(PyTorch)      |    表格数据集自动化机器学习(AutoML)包    |  [github](https://github.com/carefree0910/carefree-learn)   |\\n|   封闭域微调表格检测     |        |  [github](https://github.com/holms-ur/fine-tuning)   |\\n|   PDF表格数据提取工具     |        |   [github](https://github.com/camelot-dev/camelot)  |\\n|     TaBERT理解表格数据查询的新模型   |        |  [paper](https://scontent-hkt1-1xxfbcdnnet/v/t398562-6/106708899_597765107810230_1899215558892880563_npdf?_nc_cat=107&_nc_sid=ae5e01&_nc_ohc=4sN3TJwewSIAX8iliBD&_nc_ht=scontent-hkt1-1xx&oh=eccb9795f027ff63be61ff4a5e337c02&oe=5F316505)   |\\n| 表格处理 | Awesome-Table-Recognition | [github](https://github.com/cv-small-snails/Awesome-Table-Recognition)|\\n\\n\\n\\n# 文本匹配\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|    句子、QA相似度匹配MatchZoo    |   文本相似度匹配算法的集合，包含多个深度学习的方法，值得尝试。    |    [github](https://github.com/NTMC-Community/MatchZoo)  |\\n|    中文问题句子相似度计算比赛及方案汇总    |        |   [github](https://github.com/ShuaichiLi/Chinese-sentence-similarity-task)  |\\n|    similarity相似度计算工具包    |   java编写,用于词语、短语、句子、词法分析、情感分析、语义分析等相关的相似度计算    |  [github](https://github.com/shibing624/similarity)   |\\n|    中文词语相似度计算方法    |  综合了同义词词林扩展版与知网（Hownet）的词语相似度计算方法，词汇覆盖更多、结果更准确。      |    [gihtub](https://github.com/yaleimeng/Final_word_Similarity) |\\n|    Python字符串相似性算法库    |        |    [github](https://github.com/luozhouyang/python-string-similarity) |\\n|    基于Siamese bilstm模型的相似句子判定模型,提供训练数据集和测试数据集    |    提供了10万个训练样本    | [github](https://github.com/liuhuanyong/SiameseSentenceSimilarity)    |\\n\\n\\n# 文本数据增强\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|    中文NLP数据增强（EDA）工具  |        |    [github](https://github.com/zhanlaoban/eda_nlp_for_Chinese) |\\n|   英文NLP数据增强工具     |        |  [github](https://github.com/makcedward/nlpaug)   |\\n|    一键中文数据增强工具    |        | [github](https://github.com/425776024/nlpcda)   |\\n|    数据增强在机器翻译及其他nlp任务中的应用及效果    |        |  [link](https://mp.weixin.qq.com/s/_aVwSWuYho_7MUT0LuFgVA)   |\\n|    NLP数据增广资源集    |        |   [github](https://github.com/quincyliang/nlp-data-augmentation)  |\\n\\n\\n# 常用正则表达式\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   抽取email的正则表达式    |    |  已集成到 python package [cocoNLP](https://github.com/fighting41love/cocoNLP)中，欢迎试用  |\\n|   抽取phone_number    |     | 已集成到 python package [cocoNLP](https://github.com/fighting41love/cocoNLP)中，欢迎试用   |\\n|    抽取身份证号的正则表达式   |  IDCards_pattern = r\\'^([1-9]\\\\d{5}[12]\\\\d{3}(0[1-9]\\\\|1[012])(0[1-9]\\\\|[12][0-9]\\\\|3[01])\\\\d{3}[0-9xX])<br>IDs = re.findall(IDCards_pattern, text, flags=0)|   \\n  IP地址正则表达式|(25[0-5]\\\\|  2[0-4]\\\\d\\\\|  [0-1]\\\\d{2}\\\\|  [1-9]?\\\\d)\\\\.(25[0-5]\\\\|  2[0-4]\\\\d\\\\|  [0-1]\\\\d{2}\\\\|  [1-9]?\\\\d)\\\\.(25[0-5]\\\\|  2[0-4]\\\\d\\\\|  [0-1]\\\\d{2}\\\\|  [1-9]?\\\\d)\\\\.(25[0-5]\\\\|  2[0-4]\\\\d\\\\|  [0-1]\\\\d{2}\\\\|  [1-9]?\\\\d)||\\n|  腾讯QQ号正则表达式     |   \\\\[1-9]([0-9]{5,11})     |    |\\n|   国内固话号码正则表达式    |      [0-9-()（）]{7,18}  |    |\\n|   用户名正则表达式    |  [A-Za-z0-9_\\\\-\\\\u4e00-\\\\u9fa5]+      |    |\\n|    国内电话号码正则匹配（三大运营商+虚拟等）    |        |   [github](https://github.com/VincentSit/ChinaMobilePhoneNumberRegex)  |\\n|     正则表达式教程   |        |  [github](https://github.com/ziishaned/learn-regex/blob/master/translations/README-cnmd)   |\\n\\n\\n# 文本检索\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   高效模糊搜索工具     |        |  [github](https://github.com/Yggdroot/LeaderF)   |\\n|  面向各语种/任务的BERT模型大列表/搜索引擎      |        |    [link](https://bertlang.unibocconi.it/) |\\n|    Deepmatch针对推荐、广告和搜索的深度匹配模型库    |        |   [github](https://github.com/shenweichen/DeepMatch)  |\\n|    wwsearch是企业微信后台自研的全文检索引擎    |        |   [github](https://github.com/Tencent/wwsearch)  |\\n|   aili - the fastest in-memory index in the East 东半球最快并发索引     |        |    [github](https://github.com/UncP/aili) |\\n|高效的字符串匹配工具 RapidFuzz|a fast string matching library for Python and C++, which is using the string similarity calculations from FuzzyWuzzy|[github](https://github.com/maxbachmann/rapidfuzz)|\\n\\n# 阅读理解\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   高效模糊搜索工具     |        |  [github](https://github.com/Yggdroot/LeaderF)   |\\n|  面向各语种/任务的BERT模型大列表/搜索引擎      |        |    [link](https://bertlang.uniboc.coni.it) |\\n|    Deepmatch针对推荐、广告和搜索的深度匹配模型库    |        |   [github](https://github.com/shenweichen/DeepMatch)  |\\n|   allennlp阅读理解支持多种数据和模     |        |  [github](https://github.com/allenai/allennlp-reading-comprehension)   |\\n\\n\\n\\n# 情感分析\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|     方面情感分析包   |        |   [github](https://github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis)  |\\n|    awesome-nlp-sentiment-analysis    |    情感分析、情绪原因识别、评价对象和评价词抽取   |  [github](https://github.com/haiker2011/awesome-nlp-sentiment-analysis)   |\\n|    情感分析技术让智能客服更懂人类情感    |        |   [github](https://developeraliyuncom/article/761513?utm_content=g_1000124809)  |\\n\\n\\n# 事件抽取\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   中文事件抽取    |        |  [github](https://github.com/liuhuanyong/ComplexEventExtraction)  |\\n|     NLP事件提取文献资源列表   |        |  [github](https://github.com/BaptisteBlouin/EventExtractionPapers)   |\\n|    PyTorch实现的BERT事件抽取(ACE 2005 corpus)    |         | [github](https://github.com/nlpcl-lab/bert-event-extraction)   |\\n|  新闻事件线索抽取      |        |  [github](https://github.com/liuhuanyong/ImportantEventExtractor)   |\\n\\n\\n# 机器翻译\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   无道词典     |  有道词典的命令行版本，支持英汉互查和在线查询      |  [github](https://github.com/ChestnutHeng/Wudao-dict)   |\\n|NLLB|支持200+种语言任意互译的语言模型NLLB|[link](https://openbmb.github.io/BMList/list/)|\\n\\n# 数字转换\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   最好的汉字数字(中文数字)-阿拉伯数字转换工具     |        |   [github](https://github.com/Wall-ee/chinese2digits)  |\\n|  快速转化「中文数字」和「阿拉伯数字」      |        |    [github](https://github.com/HaveTwoBrush/cn2an) |\\n|    将自然语言数字串解析转换为整数和浮点数    |        |   [github](https://github.com/jaidevd/numerizer)  |\\n\\n\\n# 指代消解\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|    中文指代消解数据    |        |   [github](https://github.com/CLUEbenchmark/CLUEWSC2020) <br>[baidu ink](https://panbaiducom/s/1gKP_Mj-7KVfFWpjYvSvAAA)  code a0qq |\\n\\n\\n# 文本聚类\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|     TextCluster短文本聚类预处理模块 Short text cluster   |        |    [github](https://github.com/RandyPen/TextCluster) |\\n\\n\\n# 文本分类\\n\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|    NeuralNLP-NeuralClassifier腾讯开源深度学习文本分类工具    |        |    [github](https://github.com/Tencent/NeuralNLP-NeuralClassifier) |\\n\\n\\n# 知识推理\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   GraphbrainAI开源软件库和科研工具，目的是促进自动意义提取和文本理解以及知识的探索和推断     |        |    [github](https://github.com/graphbrain/graphbrain) |\\n|    (哈佛)讲因果推理的免费书    |        |  [pdf](https://cdn1sphharvardedu/wp-content/uploads/sites/1268/2019/10/ci_hernanrobins_23oct19pdf)   |\\n\\n# 可解释自然语言处理\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   文本机器学习模型最先进解释器库     |        |  [github](https://github.com/interpretml/interpret-text)   |\\n\\n\\n# 文本攻击\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|     TextAttack自然语言处理模型对抗性攻击框架   |        |    [github](https://github.com/QData/TextAttack) |\\n|OpenBackdoor: 文本后门攻防工具包|       OpenBackdoor基于Python和PyTorch开发，可用于复现、评估和开发文本后门攻防的相关算法     |    [github](https://github.com/thunlp/OpenBackdoor)|\\n\\n# 文本可视化\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|     Scattertext 文本可视化(python)   |        |   [github](https://github.com/JasonKessler/scattertext)  |\\n|     whatlies词向量交互可视化   |        | [spacy工具](https://spacyio/universe/project/whatlies)  |\\n|   PySS3面向可解释AI的SS3文本分类器机器可视化工具     |        |   [github](https://github.com/sergioburdisso/pyss3)  |\\n|     用记事本渲染3D图像   |        | [github](https://github.com/khalladay/render-with-notepad)    |\\n|    attnvisGPT2、BERT等transformer语言模型注意力交互可视化    |        |   [github](https://github.com/SIDN-IAP/attnvis)  |\\n|    Texthero文本数据高效处理包    |   包括预处理、关键词提取、命名实体识别、向量空间分析、文本可视化等     |  [github](https://github.com/jbesomi/texthero)   |\\n\\n# 文本标注工具\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   NLP标注平台综述     |        |   [github](https://github.com/alvations/annotate-questionnaire)  |\\n|   brat rapid annotation tool 序列标注工具     |        |   [link](http://brat.nlplab.org/index.html)  |\\n|     Poplar网页版自然语言标注工具   |        |    [github](https://github.com/synyi/poplar) |\\n|   LIDA轻量交互式对话标注工具     |        |  [github](https://github.com/Wluper/lida)   |\\n|    doccano基于网页的开源协同多语言文本标注工具    |        |   [github](https://github.com/doccano/doccano)  |\\n|     Datasaurai 在线数据标注工作流管理工具   |        |    [link](https://datasaurai.gitbook.io/datasaur/) |\\n\\n# 语言检测\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|  langid     |   97种语言检测     |  [https://github.com/saffsd/langid.py](https://github.com/saffsd/langid.py)  |\\n|   langdetect    |   语言检测     |  [https://code.google.com/archive/p/language-detection/](https://code.google.com/archive/p/language-detection/)  |\\n\\n# 综合工具\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   jieba    |        |  [jieba](https://github.com/fxsjy/jieba)  |\\n|  hanlp     |        |   [hanlp](https://github.com/hankcs/pyhanlp) |\\n|    nlp4han    |  中文自然语言处理工具集(断句/分词/词性标注/组块/句法分析/语义分析/NER/N元语法/HMM/代词消解/情感分析/拼写检    |   [github](https://github.com/kidden/nlp4han)  |\\n|    仇恨言论检测进展    |        |   [link](https://ai.facebook.com/blog/ai-advances-to-better-detect-hate-speech)  |\\n|   基于Pytorch的Bert应用    |    包括命名实体识别、情感分析、文本分类以及文本相似度等    |  [github](https://github.com/rsanshierli/EasyBert)   |\\n|    nlp4han中文自然语言处理工具集    |   断句/分词/词性标注/组块/句法分析/语义分析/NER/N元语法/HMM/代词消解/情感分析/拼写检查     |   [github](https://github.com/kidden/nlp4han)  |\\n|    一些关于自然语言的基本模型    |        |  [github](https://github.com/lpty/nlp_base)   |\\n|     用BERT进行序列标记和文本分类的模板代码   |        |  [github](https://github.com/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification)|\\n|   jieba_fast 加速版的jieba     |        |  [github](https://github.com/deepcs233/jieba_fast)   |\\n|    StanfordNLP     |   纯Python版自然语言处理包     |  [link](https://stanford.nlp.github.io/stanfordnlp/)   |\\n|     Python口语自然语言处理工具集(英文)   |        |  [github](https://github.com/gooofy/py-nltools)   |\\n|    PreNLP自然语言预处理库    |        |  [github](https://github.com/lyeoni/prenlp)   |\\n|    nlp相关的一些论文及代码    |  包括主题模型、词向量(Word Embedding)、命名实体识别(NER)、文本分类(Text Classificatin)、文本生成(Text Generation)、文本相似性(Text Similarity)计算等，涉及到各种与nlp相关的算法，基于keras和tensorflow      |    [github](https://github.com/msgi/nlp-journey) |\\n|  Python文本挖掘/NLP实战示例      |        |   [github](https://github.com/kavgan/nlp-in-practice)  |\\n|   Forte灵活强大的自然语言处理pipeline工具集     |        |    [github](https://github.com/asyml/forte) |\\n|   stanza斯坦福团队NLP工具     |  可处理六十多种语言   |    [github](https://github.com/stanfordnlp/stanza) |\\n|   Fancy-NLP用于建设商品画像的文本知识挖掘工具     |        |   [github](https://github.com/boat-group/fancy-nlp)  |\\n|    全面简便的中文 NLP 工具包    |        |   [github](https://github.com/dongrixinyu/JioNLP)  |\\n|   工业界常用基于DSSM向量化召回pipeline复现     |        |   [github](https://github.com/wangzhegeek/DSSM-Lookalike)  |\\n|    Texthero文本数据高效处理包    |   包括预处理、关键词提取、命名实体识别、向量空间分析、文本可视化等     |  [github](https://github.com/jbesomi/texthero)   |\\n|    nlpgnn图神经网络自然语言处理工具箱    |        |  [github](https://github.com/kyzhouhzau/NLPGNN)   |\\n|    Macadam   |  以Tensorflow(Keras)和bert4keras为基础，专注于文本分类、序列标注和关系抽取的自然语言处理工具包     |    [github](https://github.com/yongzhuo/Macadam) |\\n|    LineFlow面向所有深度学习框架的NLP数据高效加载器    |        |   [github](https://github.com/tofunlp/lineflow)  |\\n|Arabica：Python文本数据探索性分析工具包||[github](https://github.com/PetrKorab/Arabica)|\\n|Python 压力测试工具：SMSBoom||[github](github.com/WhaleFell/SMSBoom)|\\n\\n# 有趣搞笑工具\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   汪峰歌词生成器    |        |  [phunterlau/wangfeng-rnn](https://github.com/phunterlau/wangfeng-rnn)  |\\n|  女友 情感波动分析     |        |  [github](https://github.com/CasterWx/python-girlfriend-mood/)  |\\n|   NLP太难了系列    |        |   [github](https://github.com/fighting41love/hardNLP)  |\\n|  变量命名神器     |        |  [github](https://github.com/unbug/codelf) [link](https://unbug.github.io/codelf/)  |\\n|     图片文字去除，可用于漫画翻译   |        |  [github](https://github.com/yu45020/Text_Segmentation_Image_Inpainting)   |\\n|     CoupletAI - 对联生成   |   基于CNN+Bi-LSTM+Attention 的自动对对联系统     |  [github](https://github.com/WiseDoge/CoupletAI)   |\\n|   用神经网络符号推理求解复杂数学方程     |        |    [github](https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/) |\\n|   基于14W歌曲知识库的问答机器人    |     功能包括歌词接龙，已知歌词找歌曲以及歌曲歌手歌词三角关系的问答   |    [github](https://github.com/liuhuanyong/MusicLyricChatbot) |\\n|    COPE - 格律诗编辑程序    |        |  [github](https://github.com/LingDong-/cope)   |\\n|Paper2GUI | 一款面向普通人的AI桌面APP工具箱，免安装即开即用，已支持18+AI模型，内容涵盖语音合成、视频补帧、视频超分、目标检测、图片风格化、OCR识别等领域 |   [github](https://github.com/Baiyuetribe/paper2gui) |  \\n|礼貌程度估算器（使用新浪微博数据训练）|| [github](https://github.com/tslmy/politeness-estimator) [paper](https://dl.acm.org/doi/abs/10.1145/3415190)|\\n|草蟒（Python 中文版）入门指南|中文编程语言|[homepage](https://www.grasspy.cn/zwdocs/grasspy-start/day1/)  [gitee](https://gitee.com/laowu2019_admin/zwdocs)|\\n\\n\\n\\n# 课程报告面试等\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   自然语言处理报告     |        |  [link](https://static.aminer.cn/misc/article/nlppdf)  |\\n|    知识图谱报告   |        |   [link](https://www.aminer.cn/research_report/5c3d5a8709%20e961951592a49d?download=true&pathname=knowledgegraphpdf) |\\n|   数据挖掘报告   |        |  [link](https://www.aminer.cn/research_report/5c3d5a5cecb160952fa10b76?download=true&pathname=dataminingpdf)  |\\n|   自动驾驶报告    |        |  [link](https://static.aminer.cn/misc/article/selfdrivingpdf)  |\\n|   机器翻译报告    |        |  [link](https://static.aminer.cn/misc/article/translationpdf)  |\\n|    区块链报告   |        |  [link](https://static.aminer.cn/misc/article/blockchain_publicpdf)  |\\n|   机器人报告    |        | [link](https://static.aminer.cn/misc/article/robotics_betapdf)   |\\n|   计算机图形学报告    |        |  [link](https://static.aminer.cn/misc/article/cgpdf)  |\\n|  3D打印报告    |        |  [link](https://static.aminer.cn/misc/article/3dpdf)  |\\n|   人脸识别报告    |        |  [link](https://static.aminer.cn/misc/article/facerecognitionpdf)  |\\n|   人工智能芯片报告    |        |  [link](https://static.aminer.cn/misc/article/aichippdf)  |\\n|   cs224n深度学习自然语言处理课程    |        |  [link](http//web.stanford.edu/class/cs224n/) 课程中模型的pytorch实现 [link](https://github.com/DSKSD/DeepNLP-models-Pytorch)   |\\n|   面向深度学习研究人员的自然语言处理实例教程     |        |  [github](https://github.com/graykode/nlp-tutorial)  |\\n|   《Natural Language Processing》by Jacob Eisenstein     |        |   [github](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notespdf)  |\\n|     ML-NLP    | 机器学习(Machine Learning)、NLP面试中常考到的知识点和代码实现       |    [github](https://github.com/NLP-LOVE/ML-NLP) |\\n|      NLP任务示例项目代码集  |        |   [github](https://github.com/explosion/projects)  |\\n|     2019年NLP亮点回顾   |        |   [download](https://panbaiducom/s/1h5gEPUhvY1HkUVc32eeX4w)  |\\n|   nlp-recipes微软出品--自然语言处理最佳实践和范例     |        |   [github](https://github.com/microsoft/nlp-recipes)  |\\n|    面向深度学习研究人员的自然语言处理实例教程    |        |   [github](https://github.com/graykode/nlp-tutorial)  |\\n|   Transfer Learning in Natural Language Processing (NLP)     |        |    [youtube](https://www.youtube.com/watch?v=ly0TRNr7I_M) |\\n|《机器学习系统》图书|  |  [link](https://openmlsys.github.io/)  [github](https://github.com/fighting41love/openmlsys-zh) |\\n\\n\\n# 比赛\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|    复盘所有NLP比赛的TOP方案    |        |   [github](https://github.com/zhpmatrix/nlp-competitions-list-review)  |\\n|   2019年百度的三元组抽取比赛，“科学空间队”源码(第7名)     |        |   [github](https://github.com/bojone/kg-2019)  |\\n\\n\\n# 金融自然语言处理\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   BDCI2019金融负面信息判定     |        |   [github](https://github.com/A-Rain/BDCI2019-Negative_Finance_Info_Judge)  |\\n|     开源的金融投资数据提取工具    |        |    [github](https://github.com/PKUJohnson/OpenData) |\\n|    金融领域自然语言处理研究资源大列表    |        |    [github](https://github.com/icoxfog417/awesome-financial-nlp) |\\n|   基于金融-司法领域(兼有闲聊性质)的聊天机器人     |        |   [github](https://github.com/charlesXu86/Chatbot_CN)  |\\n|小型金融知识图谱构流程示范| |[github](github.com/jm199504/Financial-Knowledge-Graphs)|\\n\\n# 医疗自然语言处理\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|   中文医学NLP公开资源整理     |        |   [github](https://github.com/GanjinZero/awesome_Chinese_medical_NLP)  |\\n|    spaCy 医学文本挖掘与信息提取    |        |  [github](https://github.com/NLPatVCU/medaCy)   |\\n|    构建医疗实体识别的模型 |   包含词典和语料标注，基于python    |  [github](https://github.com/yixiu00001/LSTM-CRF-medical)   |\\n|     基于医疗领域知识图谱的问答系统   |        |   [github](https://github.com/zhihao-chen/QASystemOnMedicalGraph) 该repo参考了[github](https://github.com/liuhuanyong/QASystemOnMedicalKG)   |\\n|     Chinese medical dialogue data 中文医疗对话数据集   |        |   [github](https://github.com/Toyhom/Chinese-medical-dialogue-data)  |\\n|    一个大规模医疗对话数据集    |   包含110万医学咨询，400万条医患对话    |    [github](https://github.com/UCSD-AI4H/Medical-Dialogue-System) |\\n|   新冠肺炎相关数据     |  新冠及其他类型肺炎中文医疗对话数据集；清华大学等机构的开放数据源（COVID-19）   | [github](https://www。aminer。cn/data-covid19/)<br>  [github](https://github.com/UCSD-AI4H/COVID-Dialogue) |\\n\\n\\n# 法律自然语言处理\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|    Blackstone面向非结构化法律文本的spaCy pipeline和NLP模型    |        |    [github](https://github.com/ICLRandD/Blackstone) |\\n|   法务智能文献资源列表     |        |  [github](https://github.com/thunlp/LegalPapers)   |\\n|   基于金融-司法领域(兼有闲聊性质)的聊天机器人     |        |   [github](https://github.com/charlesXu86/Chatbot_CN)  |\\n|   罪名法务名词及分类模型    |    包含856项罪名知识图谱, 基于280万罪名训练库的罪名预测,基于20W法务问答对的13类问题分类与法律资讯问答功能    |    [github](https://github.com/liuhuanyong/CrimeKgAssitant)     |\\n\\n\\n# 其他\\n\\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|  phone     |     中国手机归属地查询    |  [ls0f/phone](https://github.com/ls0f/phone)  |\\n|   phone    |    国际手机、电话归属地查询    |   [AfterShip/phone](https://github.com/AfterShip/phone) |\\n|    ngender   |   根据名字判断性别     |  [observerss/ngender](https://github.com/observerss/ngender)  |\\n|    中文对比英文自然语言处理NLP的区别综述  |        |   [link](https://mp.weixin.qq.com/s/LQU_HJ4q74lL5oCIk7w5RA)  |\\n|  各大公司内部里大牛分享的技术文档 PDF 或者 PPT      |        |   [github](https://github.com/0voice/from_coder_to_expert)  |\\n|   comparxiv 用于比较arXiv上两提交版本差异的命令     |        |  [pypi](https://pypiorg/project/comparxiv/)   |\\n|     CHAMELEON深度学习新闻推荐系统元架构   |        | [github](https://github.com/gabrielspmoreira/chameleon_recsys)    |\\n|    简历自动筛选系统    |        |   [github](https://github.com/JAIJANYANI/Automated-Resume-Screening-System)  |\\n|    Python实现的多种文本可读性评价指标    |        |    [github](https://github.com/cdimascio/py-readability-metrics) |\\n\\n\\n\\n\\n# 备注\\n\\n涉及内容包括但不限于：**中英文敏感词、语言检测、中外手机/电话归属地/运营商查询、名字推断性别、手机号抽取、身份证抽取、邮箱抽取、中日文人名库、中文缩写库、拆字词典、词汇情感值、停用词、反动词表、暴恐词表、繁简体转换、英文模拟中文发音、汪峰歌词生成器、职业名称词库、同义词库、反义词库、否定词库、汽车品牌词库、汽车零件词库、连续英文切割、各种中文词向量、公司名字大全、古诗词库、IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库、中文聊天语料、中文谣言数据、百度中文问答数据集、句子相似度匹配算法集合、bert资源、文本生成&摘要相关工具、cocoNLP信息抽取工具、国内电话号码正则匹配、清华大学XLORE:中英文跨语言百科知识图谱、清华大学人工智能技术系列报告、自然语言生成、NLU太难了系列、自动对联数据及机器人、用户名黑名单列表、罪名法务名词及分类模型、微信公众号语料、cs224n深度学习自然语言处理课程、中文手写汉字识别、中文自然语言处理 语料/数据集、变量命名神器、分词语料库+代码、任务型对话英文数据集、ASR 语音数据集 + 基于深度学习的中文语音识别系统、笑声检测器、Microsoft多语言数字/单位/如日期时间识别包、中华新华字典数据库及api(包括常用歇后语、成语、词语和汉字)、文档图谱自动生成、SpaCy 中文模型、Common Voice语音识别数据集新版、神经网络关系抽取、基于bert的命名实体识别、关键词(Keyphrase)抽取包pke、基于医疗领域知识图谱的问答系统、基于依存句法与语义角色标注的事件三元组抽取、依存句法分析4万句高质量标注数据、cnocr：用来做中文OCR的Python3包、中文人物关系知识图谱项目、中文nlp竞赛项目及代码汇总、中文字符数据、speech-aligner: 从“人声语音”及其“语言文本”产生音素级别时间对齐标注的工具、AmpliGraph: 知识图谱表示学习(Python)库：知识图谱概念链接预测、Scattertext 文本可视化(python)、语言/知识表示工具：BERT & ERNIE、中文对比英文自然语言处理NLP的区别综述、Synonyms中文近义词工具包、HarvestText领域自适应文本挖掘工具（新词发现-情感分析-实体链接等）、word2word：(Python)方便易用的多语言词-词对集：62种语言/3,564个多语言对、语音识别语料生成工具：从具有音频/字幕的在线视频创建自动语音识别(ASR)语料库、构建医疗实体识别的模型（包含词典和语料标注）、单文档非监督的关键词抽取、Kashgari中使用gpt-2语言模型、开源的金融投资数据提取工具、文本自动摘要库TextTeaser: 仅支持英文、人民日报语料处理工具集、一些关于自然语言的基本模型、基于14W歌曲知识库的问答尝试--功能包括歌词接龙and已知歌词找歌曲以及歌曲歌手歌词三角关系的问答、基于Siamese bilstm模型的相似句子判定模型并提供训练数据集和测试数据集、用Transformer编解码模型实现的根据Hacker News文章标题自动生成评论、用BERT进行序列标记和文本分类的模板代码、LitBank：NLP数据集——支持自然语言处理和计算人文学科任务的100部带标记英文小说语料、百度开源的基准信息抽取系统、虚假新闻数据集、Facebook: LAMA语言模型分析，提供Transformer-XL/BERT/ELMo/GPT预训练语言模型的统一访问接口、CommonsenseQA：面向常识的英文QA挑战、中文知识图谱资料、数据及工具、各大公司内部里大牛分享的技术文档 PDF 或者 PPT、自然语言生成SQL语句（英文）、中文NLP数据增强（EDA）工具、英文NLP数据增强工具 、基于医药知识图谱的智能问答系统、京东商品知识图谱、基于mongodb存储的军事领域知识图谱问答项目、基于远监督的中文关系抽取、语音情感分析、中文ULMFiT-情感分析-文本分类-语料及模型、一个拍照做题程序、世界各国大规模人名库、一个利用有趣中文语料库 qingyun 训练出来的中文聊天机器人、中文聊天机器人seqGAN、省市区镇行政区划数据带拼音标注、教育行业新闻语料库包含自动文摘功能、开放了对话机器人-知识图谱-语义理解-自然语言处理工具及数据、中文知识图谱：基于百度百科中文页面-抽取三元组信息-构建中文知识图谱、masr: 中文语音识别-提供预训练模型-高识别率、Python音频数据增广库、中文全词覆盖BERT及两份阅读理解数据、ConvLab：开源多域端到端对话系统平台、中文自然语言处理数据集、基于最新版本rasa搭建的对话系统、基于TensorFlow和BERT的管道式实体及关系抽取、一个小型的证券知识图谱/知识库、复盘所有NLP比赛的TOP方案、OpenCLaP：多领域开源中文预训练语言模型仓库、UER：基于不同语料+编码器+目标任务的中文预训练模型仓库、中文自然语言处理向量合集、基于金融-司法领域(兼有闲聊性质)的聊天机器人、g2pC：基于上下文的汉语读音自动标记模块、Zincbase 知识图谱构建工具包、诗歌质量评价/细粒度情感诗歌语料库、快速转化「中文数字」和「阿拉伯数字」、百度知道问答语料库、基于知识图谱的问答系统、jieba_fast 加速版的jieba、正则表达式教程、中文阅读理解数据集、基于BERT等最新语言模型的抽取式摘要提取、Python利用深度学习进行文本摘要的综合指南、知识图谱深度学习相关资料整理、维基大规模平行文本语料、StanfordNLP 0.2.0：纯Python版自然语言处理包、NeuralNLP-NeuralClassifier：腾讯开源深度学习文本分类工具、端到端的封闭域对话系统、中文命名实体识别：NeuroNER vs. BertNER、新闻事件线索抽取、2019年百度的三元组抽取比赛：“科学空间队”源码、基于依存句法的开放域文本知识三元组抽取和知识库构建、中文的GPT2训练代码、ML-NLP - 机器学习(Machine Learning)NLP面试中常考到的知识点和代码实现、nlp4han:中文自然语言处理工具集(断句/分词/词性标注/组块/句法分析/语义分析/NER/N元语法/HMM/代词消解/情感分析/拼写检查、XLM：Facebook的跨语言预训练语言模型、用基于BERT的微调和特征提取方法来进行知识图谱百度百科人物词条属性抽取、中文自然语言处理相关的开放任务-数据集-当前最佳结果、CoupletAI - 基于CNN+Bi-LSTM+Attention 的自动对对联系统、抽象知识图谱、MiningZhiDaoQACorpus - 580万百度知道问答数据挖掘项目、brat rapid annotation tool: 序列标注工具、大规模中文知识图谱数据：1.4亿实体、数据增强在机器翻译及其他nlp任务中的应用及效果、allennlp阅读理解:支持多种数据和模型、PDF表格数据提取工具 、 Graphbrain：AI开源软件库和科研工具，目的是促进自动意义提取和文本理解以及知识的探索和推断、简历自动筛选系统、基于命名实体识别的简历自动摘要、中文语言理解测评基准，包括代表性的数据集&基准模型&语料库&排行榜、树洞 OCR 文字识别 、从包含表格的扫描图片中识别表格和文字、语声迁移、Python口语自然语言处理工具集(英文)、 similarity：相似度计算工具包，java编写、海量中文预训练ALBERT模型 、Transformers 2.0 、基于大规模音频数据集Audioset的音频增强 、Poplar：网页版自然语言标注工具、图片文字去除，可用于漫画翻译 、186种语言的数字叫法库、Amazon发布基于知识的人-人开放领域对话数据集 、中文文本纠错模块代码、繁简体转换 、 Python实现的多种文本可读性评价指标、类似于人名/地名/组织机构名的命名体识别数据集 、东南大学《知识图谱》研究生课程(资料)、. 英文拼写检查库 、 wwsearch是企业微信后台自研的全文检索引擎、CHAMELEON：深度学习新闻推荐系统元架构 、 8篇论文梳理BERT相关模型进展与反思、DocSearch：免费文档搜索引擎、 LIDA：轻量交互式对话标注工具 、aili - the fastest in-memory index in the East 东半球最快并发索引 、知识图谱车音工作项目、自然语言生成资源大全 、中日韩分词库mecab的Python接口库、中文文本摘要/关键词提取、汉字字符特征提取器 (featurizer)，提取汉字的特征（发音特征、字形特征）用做深度学习的特征、中文生成任务基准测评 、中文缩写数据集、中文任务基准测评 - 代表性的数据集-基准(预训练)模型-语料库-baseline-工具包-排行榜、PySS3：面向可解释AI的SS3文本分类器机器可视化工具 、中文NLP数据集列表、COPE - 格律诗编辑程序、doccano：基于网页的开源协同多语言文本标注工具 、PreNLP：自然语言预处理库、简单的简历解析器，用来从简历中提取关键信息、用于中文闲聊的GPT2模型：GPT2-chitchat、基于检索聊天机器人多轮响应选择相关资源列表(Leaderboards、Datasets、Papers)、(Colab)抽象文本摘要实现集锦(教程 、词语拼音数据、高效模糊搜索工具、NLP数据增广资源集、微软对话机器人框架 、 GitHub Typo Corpus：大规模GitHub多语言拼写错误/语法错误数据集、TextCluster：短文本聚类预处理模块 Short text cluster、面向语音识别的中文文本规范化、BLINK：最先进的实体链接库、BertPunc：基于BERT的最先进标点修复模型、Tokenizer：快速、可定制的文本词条化库、中文语言理解测评基准，包括代表性的数据集、基准(预训练)模型、语料库、排行榜、spaCy 医学文本挖掘与信息提取 、 NLP任务示例项目代码集、 python拼写检查库、chatbot-list - 行业内关于智能客服、聊天机器人的应用和架构、算法分享和介绍、语音质量评价指标(MOSNet, BSSEval, STOI, PESQ, SRMR)、 用138GB语料训练的法文RoBERTa预训练语言模型 、BERT-NER-Pytorch：三种不同模式的BERT中文NER实验、无道词典 - 有道词典的命令行版本，支持英汉互查和在线查询、2019年NLP亮点回顾、 Chinese medical dialogue data 中文医疗对话数据集 、最好的汉字数字(中文数字)-阿拉伯数字转换工具、 基于百科知识库的中文词语多词义/义项获取与特定句子词语语义消歧、awesome-nlp-sentiment-analysis - 情感分析、情绪原因识别、评价对象和评价词抽取、LineFlow：面向所有深度学习框架的NLP数据高效加载器、中文医学NLP公开资源整理 、MedQuAD：(英文)医学问答数据集、将自然语言数字串解析转换为整数和浮点数、Transfer Learning in Natural Language Processing (NLP) 、面向语音识别的中文/英文发音辞典、Tokenizers：注重性能与多功能性的最先进分词器、CLUENER 细粒度命名实体识别 Fine Grained Named Entity Recognition、 基于BERT的中文命名实体识别、中文谣言数据库、NLP数据集/基准任务大列表、nlp相关的一些论文及代码, 包括主题模型、词向量(Word Embedding)、命名实体识别(NER)、文本分类(Text Classificatin)、文本生成(Text Generation)、文本相似性(Text Similarity)计算等，涉及到各种与nlp相关的算法，基于keras和tensorflow 、Python文本挖掘/NLP实战示例、 Blackstone：面向非结构化法律文本的spaCy pipeline和NLP模型通过同义词替换实现文本“变脸” 、中文 预训练 ELECTREA 模型: 基于对抗学习 pretrain Chinese Model 、albert-chinese-ner - 用预训练语言模型ALBERT做中文NER 、基于GPT2的特定主题文本生成/文本增广、开源预训练语言模型合集、多语言句向量包、编码、标记和实现：一种可控高效的文本生成方法、 英文脏话大列表 、attnvis：GPT2、BERT等transformer语言模型注意力交互可视化、CoVoST：Facebook发布的多语种语音-文本翻译语料库，包括11种语言(法语、德语、荷兰语、俄语、西班牙语、意大利语、土耳其语、波斯语、瑞典语、蒙古语和中文)的语音、文字转录及英文译文、Jiagu自然语言处理工具 - 以BiLSTM等模型为基础，提供知识图谱关系抽取 中文分词 词性标注 命名实体识别 情感分析 新词发现 关键词 文本摘要 文本聚类等功能、用unet实现对文档表格的自动检测，表格重建、NLP事件提取文献资源列表 、 金融领域自然语言处理研究资源大列表、CLUEDatasetSearch - 中英文NLP数据集：搜索所有中文NLP数据集，附常用英文NLP数据集 、medical_NER - 中文医学知识图谱命名实体识别 、(哈佛)讲因果推理的免费书、知识图谱相关学习资料/数据集/工具资源大列表、Forte：灵活强大的自然语言处理pipeline工具集 、Python字符串相似性算法库、PyLaia：面向手写文档分析的深度学习工具包、TextFooler：针对文本分类/推理的对抗文本生成模块、Haystack：灵活、强大的可扩展问答(QA)框架、中文关键短语抽取工具**。\\n\\n<!-- \\n| 资源名（Name）      | 描述（Description） | 链接     |\\n| :---        |    :---  |          :--- |\\n|       |        |    |\\n|       |        |    |\\n|       |        |    |\\n|       |        |    |\\n|       |        |    |\\n|       |        |    |\\n|       |        |    |\\n|       |        |    |\\n|       |        |    |\\n|       |        |    |\\n|       |        |    |\\n|       |        |    |\\n|       |        |    |\\n|       |        |    |\\n|       |        |    |\\n|       |        |    | -->\\n\\n\\n<!-- <img align=\"right\" src=\"https://github-readme-stats.vercel.app/api?username=fighting41love&show_icons=true&icon_color=CE1D2D&text_color=718096&bg_color=ffffff&hide_title=true\" /> -->'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if we successfully aquire the data and clean it\n",
    "data.readme_contents[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1b7a5",
   "metadata": {},
   "source": [
    "### Takeaway\n",
    "- The readme acquired contains foreign language, we will going to drop those languages that is not in English, due to we acquired about 490 rows, we met the requirement of at least 100 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d6523",
   "metadata": {},
   "source": [
    "#### Install the package needed to detect the language\n",
    "- \\# pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34f6dfa",
   "metadata": {},
   "source": [
    "### Key takeaway\n",
    "- The most starred README data on github was collected on October 17, 2022\n",
    "- The data has 490 rows\n",
    "- We found that the readme contains foreign language, therefore, we downloaded a langdetect package for further wrangle the dataset\n",
    "- we also spot some 'none' value in our language, we will do further wrangle with that data as well\n",
    "- Following this acquire, we are going to prepare for our exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6dbfeec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data acquired\n",
    "# We are acquire the data that is cleaned up with tokenized, stemmed, and lemmatized\n",
    "# add those columns into the dataframe and create a final data frame\n",
    "df=prepare.wrangle_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "395af28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DataScienceSpecialization/courses</td>\n",
       "      <td>HTML</td>\n",
       "      <td>\\n### Data Science Specialization\\n\\nThese are...</td>\n",
       "      <td>data scienc specializationthes cours materi jo...</td>\n",
       "      <td>data science specializationthese course materi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fighting41love/funNLP</td>\n",
       "      <td>Python</td>\n",
       "      <td>&lt;center&gt;\\n    &lt;img style=\"border-radius: 0.312...</td>\n",
       "      <td>nlpdatalogocitations487redsvgdatalogohomee4bab...</td>\n",
       "      <td>nlpdatalogocitations487redsvgdatalogohomee4bab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>magento/magento2</td>\n",
       "      <td>PHP</td>\n",
       "      <td>\\n&lt;p align=\"center\"&gt;\\n&lt;a href=\"https://www.cod...</td>\n",
       "      <td>magento open sourcewelcom magento open sourc p...</td>\n",
       "      <td>magento open sourcewelcome magento open source...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>arduino/Arduino</td>\n",
       "      <td>Java</td>\n",
       "      <td>&lt;p align=\"center\"&gt;\\n\\t&lt;img src=\"http://content...</td>\n",
       "      <td>&amp;#9; arduino opensourc physic comput platform ...</td>\n",
       "      <td>&amp;#9; arduino opensource physical computing pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>elunez/eladmin</td>\n",
       "      <td>Java</td>\n",
       "      <td>&lt;h1 style=\"text-align: center\"&gt;ELADMIN 后台管理系统&lt;...</td>\n",
       "      <td>eladmin aurstargithub starsgithub fork spring ...</td>\n",
       "      <td>eladmin aurstargithub starsgithub fork spring ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>harshalbenake/hbworkspace1-100</td>\n",
       "      <td>Java</td>\n",
       "      <td># hbworkspace1-100\\n\\n(1) Name :- accelormeter...</td>\n",
       "      <td>hbworkspace11001 name accelormetersensordescri...</td>\n",
       "      <td>hbworkspace11001 name accelormetersensordescri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>rprokap/pset-9</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># pset-9\\n      CREDITS SEQUENCE              ...</td>\n",
       "      <td>pset9credit sequencenewspap headlin montagehea...</td>\n",
       "      <td>pset9credits sequencenewspaper headline montag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>konzy/mass_clone</td>\n",
       "      <td>Shell</td>\n",
       "      <td># mass_clone\\nThis is a shell script that will...</td>\n",
       "      <td>massclonethi shell script clone multipl reposi...</td>\n",
       "      <td>massclonethis shell script clone multiple repo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>biter777/countries</td>\n",
       "      <td>Go</td>\n",
       "      <td># countries\\r\\n\\r\\nCountries - ISO 3166 (ISO31...</td>\n",
       "      <td>countriescountri iso 3166 iso31661 iso3166 dig...</td>\n",
       "      <td>countriescountries iso 3166 iso31661 iso3166 d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>sayantann11/all-classification-templetes-for-ML</td>\n",
       "      <td>Python</td>\n",
       "      <td># all-classification-templetes-for-ML\\nClassif...</td>\n",
       "      <td>allclassificationtempletesformlclassif machin ...</td>\n",
       "      <td>allclassificationtempletesformlclassification ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                repo    language  \\\n",
       "2                  DataScienceSpecialization/courses        HTML   \n",
       "3                              fighting41love/funNLP      Python   \n",
       "4                                   magento/magento2         PHP   \n",
       "5                                    arduino/Arduino        Java   \n",
       "6                                     elunez/eladmin        Java   \n",
       "..                                               ...         ...   \n",
       "484                   harshalbenake/hbworkspace1-100        Java   \n",
       "485                                   rprokap/pset-9  JavaScript   \n",
       "486                                 konzy/mass_clone       Shell   \n",
       "487                               biter777/countries          Go   \n",
       "488  sayantann11/all-classification-templetes-for-ML      Python   \n",
       "\n",
       "                                       readme_contents  \\\n",
       "2    \\n### Data Science Specialization\\n\\nThese are...   \n",
       "3    <center>\\n    <img style=\"border-radius: 0.312...   \n",
       "4    \\n<p align=\"center\">\\n<a href=\"https://www.cod...   \n",
       "5    <p align=\"center\">\\n\\t<img src=\"http://content...   \n",
       "6    <h1 style=\"text-align: center\">ELADMIN 后台管理系统<...   \n",
       "..                                                 ...   \n",
       "484  # hbworkspace1-100\\n\\n(1) Name :- accelormeter...   \n",
       "485  # pset-9\\n      CREDITS SEQUENCE              ...   \n",
       "486  # mass_clone\\nThis is a shell script that will...   \n",
       "487  # countries\\r\\n\\r\\nCountries - ISO 3166 (ISO31...   \n",
       "488  # all-classification-templetes-for-ML\\nClassif...   \n",
       "\n",
       "                                               stemmed  \\\n",
       "2    data scienc specializationthes cours materi jo...   \n",
       "3    nlpdatalogocitations487redsvgdatalogohomee4bab...   \n",
       "4    magento open sourcewelcom magento open sourc p...   \n",
       "5    &#9; arduino opensourc physic comput platform ...   \n",
       "6    eladmin aurstargithub starsgithub fork spring ...   \n",
       "..                                                 ...   \n",
       "484  hbworkspace11001 name accelormetersensordescri...   \n",
       "485  pset9credit sequencenewspap headlin montagehea...   \n",
       "486  massclonethi shell script clone multipl reposi...   \n",
       "487  countriescountri iso 3166 iso31661 iso3166 dig...   \n",
       "488  allclassificationtempletesformlclassif machin ...   \n",
       "\n",
       "                                            lemmatized  \n",
       "2    data science specializationthese course materi...  \n",
       "3    nlpdatalogocitations487redsvgdatalogohomee4bab...  \n",
       "4    magento open sourcewelcome magento open source...  \n",
       "5    &#9; arduino opensource physical computing pla...  \n",
       "6    eladmin aurstargithub starsgithub fork spring ...  \n",
       "..                                                 ...  \n",
       "484  hbworkspace11001 name accelormetersensordescri...  \n",
       "485  pset9credits sequencenewspaper headline montag...  \n",
       "486  massclonethis shell script clone multiple repo...  \n",
       "487  countriescountries iso 3166 iso31661 iso3166 d...  \n",
       "488  allclassificationtempletesformlclassification ...  \n",
       "\n",
       "[365 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will going to drop 'none' value in our language column\n",
    "df=df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9947ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will going to use the installed package \n",
    "# to filter out the readme contents that is in English only\n",
    "\n",
    "# we are going to import a new libary for this\n",
    "import langdetect as ld\n",
    "\n",
    "# we created a new function to detect the non-english language in read me\n",
    "# the function will return the result when it is in english,, elso will not return the result\n",
    "def is_en(txt):\n",
    "    try:\n",
    "        return ld.detect(txt)=='en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# we applied the function we created \n",
    "df = df[df['readme_contents'].apply(is_en)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce73ddad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 291 entries, 2 to 488\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   repo             291 non-null    object\n",
      " 1   language         291 non-null    object\n",
      " 2   readme_contents  291 non-null    object\n",
      " 3   stemmed          291 non-null    object\n",
      " 4   lemmatized       291 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 13.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# check how many \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26fa2f4",
   "metadata": {},
   "source": [
    "# Overall Wrangle Takeaway\n",
    "- The data aquired on October 17th, 2022 with 490 rows\n",
    "- The data contains 'none' value in the language column, we dropped all 'none' values \n",
    "- The data contains foreign language in the readme contents column, we drop all those non-english values\n",
    "- We finalized with 291 rows, and ready for exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47074c",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1a39fb",
   "metadata": {},
   "source": [
    "- our dataframe should contains numerical data for better exploration, therefore, we are going to created some columns that able to represent the overall struture of the dataframe. we picked df.lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4eb6f46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>word_list</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>non_single_words</th>\n",
       "      <th>word_count_simple</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_count</th>\n",
       "      <th>non_single_count</th>\n",
       "      <th>percent_unique</th>\n",
       "      <th>percent_repeat</th>\n",
       "      <th>percent_one_word</th>\n",
       "      <th>percent_non_single</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DataScienceSpecialization/courses</td>\n",
       "      <td>HTML</td>\n",
       "      <td>\\n### Data Science Specialization\\n\\nThese are...</td>\n",
       "      <td>data scienc specializationthes cours materi jo...</td>\n",
       "      <td>data science specializationthese course materi...</td>\n",
       "      <td>[data, science, specializationthese, course, m...</td>\n",
       "      <td>[data, science, specializationthese, course, m...</td>\n",
       "      <td>[data, science, course, material, john, data, ...</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>magento/magento2</td>\n",
       "      <td>PHP</td>\n",
       "      <td>\\n&lt;p align=\"center\"&gt;\\n&lt;a href=\"https://www.cod...</td>\n",
       "      <td>magento open sourcewelcom magento open sourc p...</td>\n",
       "      <td>magento open sourcewelcome magento open source...</td>\n",
       "      <td>[magento, open, sourcewelcome, magento, open, ...</td>\n",
       "      <td>[magento, open, sourcewelcome, source, project...</td>\n",
       "      <td>[magento, open, sourcewelcome, magento, open, ...</td>\n",
       "      <td>334</td>\n",
       "      <td>331</td>\n",
       "      <td>218</td>\n",
       "      <td>331</td>\n",
       "      <td>0.658610</td>\n",
       "      <td>0.341390</td>\n",
       "      <td>0.784404</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>arduino/Arduino</td>\n",
       "      <td>Java</td>\n",
       "      <td>&lt;p align=\"center\"&gt;\\n\\t&lt;img src=\"http://content...</td>\n",
       "      <td>&amp;#9; arduino opensourc physic comput platform ...</td>\n",
       "      <td>&amp;#9; arduino opensource physical computing pla...</td>\n",
       "      <td>[9, arduino, opensource, physical, computing, ...</td>\n",
       "      <td>[9, arduino, opensource, physical, computing, ...</td>\n",
       "      <td>[9, arduino, opensource, physical, computing, ...</td>\n",
       "      <td>221</td>\n",
       "      <td>220</td>\n",
       "      <td>170</td>\n",
       "      <td>168</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.763636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bloominstituteoftechnology/Preprocessing-I</td>\n",
       "      <td>CSS</td>\n",
       "      <td># Preprocessing I: Digital Resume\\n\\nFor this ...</td>\n",
       "      <td>preprocess digit resumefor project build digit...</td>\n",
       "      <td>preprocessing digital resumefor project buildi...</td>\n",
       "      <td>[preprocessing, digital, resumefor, project, b...</td>\n",
       "      <td>[preprocessing, digital, resumefor, project, b...</td>\n",
       "      <td>[preprocessing, digital, project, building, di...</td>\n",
       "      <td>302</td>\n",
       "      <td>293</td>\n",
       "      <td>191</td>\n",
       "      <td>277</td>\n",
       "      <td>0.651877</td>\n",
       "      <td>0.348123</td>\n",
       "      <td>0.706806</td>\n",
       "      <td>0.945392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>magento/magento2</td>\n",
       "      <td>PHP</td>\n",
       "      <td>\\n&lt;p align=\"center\"&gt;\\n&lt;a href=\"https://www.cod...</td>\n",
       "      <td>magento open sourcewelcom magento open sourc p...</td>\n",
       "      <td>magento open sourcewelcome magento open source...</td>\n",
       "      <td>[magento, open, sourcewelcome, magento, open, ...</td>\n",
       "      <td>[magento, open, sourcewelcome, source, project...</td>\n",
       "      <td>[magento, open, sourcewelcome, magento, open, ...</td>\n",
       "      <td>334</td>\n",
       "      <td>331</td>\n",
       "      <td>218</td>\n",
       "      <td>331</td>\n",
       "      <td>0.658610</td>\n",
       "      <td>0.341390</td>\n",
       "      <td>0.784404</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>harshalbenake/hbworkspace1-100</td>\n",
       "      <td>Java</td>\n",
       "      <td># hbworkspace1-100\\n\\n(1) Name :- accelormeter...</td>\n",
       "      <td>hbworkspace11001 name accelormetersensordescri...</td>\n",
       "      <td>hbworkspace11001 name accelormetersensordescri...</td>\n",
       "      <td>[hbworkspace11001, name, accelormetersensordes...</td>\n",
       "      <td>[hbworkspace11001, name, accelormetersensordes...</td>\n",
       "      <td>[hbworkspace11001, name, accelormetersensordes...</td>\n",
       "      <td>503</td>\n",
       "      <td>503</td>\n",
       "      <td>361</td>\n",
       "      <td>503</td>\n",
       "      <td>0.717694</td>\n",
       "      <td>0.282306</td>\n",
       "      <td>0.861496</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>rprokap/pset-9</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># pset-9\\n      CREDITS SEQUENCE              ...</td>\n",
       "      <td>pset9credit sequencenewspap headlin montagehea...</td>\n",
       "      <td>pset9credits sequencenewspaper headline montag...</td>\n",
       "      <td>[pset9credits, sequencenewspaper, headline, mo...</td>\n",
       "      <td>[pset9credits, sequencenewspaper, headline, mo...</td>\n",
       "      <td>[pset9credits, sequencenewspaper, headline, mo...</td>\n",
       "      <td>12801</td>\n",
       "      <td>11927</td>\n",
       "      <td>4354</td>\n",
       "      <td>11927</td>\n",
       "      <td>0.365054</td>\n",
       "      <td>0.634946</td>\n",
       "      <td>0.697290</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>konzy/mass_clone</td>\n",
       "      <td>Shell</td>\n",
       "      <td># mass_clone\\nThis is a shell script that will...</td>\n",
       "      <td>massclonethi shell script clone multipl reposi...</td>\n",
       "      <td>massclonethis shell script clone multiple repo...</td>\n",
       "      <td>[massclonethis, shell, script, clone, multiple...</td>\n",
       "      <td>[massclonethis, shell, script, clone, multiple...</td>\n",
       "      <td>[shell, script, clone, multiple, repositoriest...</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>77</td>\n",
       "      <td>91</td>\n",
       "      <td>0.726415</td>\n",
       "      <td>0.273585</td>\n",
       "      <td>0.779221</td>\n",
       "      <td>0.858491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>biter777/countries</td>\n",
       "      <td>Go</td>\n",
       "      <td># countries\\r\\n\\r\\nCountries - ISO 3166 (ISO31...</td>\n",
       "      <td>countriescountri iso 3166 iso31661 iso3166 dig...</td>\n",
       "      <td>countriescountries iso 3166 iso31661 iso3166 d...</td>\n",
       "      <td>[countriescountries, iso, 3166, iso31661, iso3...</td>\n",
       "      <td>[countriescountries, iso, 3166, iso31661, iso3...</td>\n",
       "      <td>[iso, iso3166, digit, alpha2, alpha3, country,...</td>\n",
       "      <td>536</td>\n",
       "      <td>529</td>\n",
       "      <td>226</td>\n",
       "      <td>448</td>\n",
       "      <td>0.427221</td>\n",
       "      <td>0.572779</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.846881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>sayantann11/all-classification-templetes-for-ML</td>\n",
       "      <td>Python</td>\n",
       "      <td># all-classification-templetes-for-ML\\nClassif...</td>\n",
       "      <td>allclassificationtempletesformlclassif machin ...</td>\n",
       "      <td>allclassificationtempletesformlclassification ...</td>\n",
       "      <td>[allclassificationtempletesformlclassification...</td>\n",
       "      <td>[allclassificationtempletesformlclassification...</td>\n",
       "      <td>[machine, learning, classification, tutorial, ...</td>\n",
       "      <td>1880</td>\n",
       "      <td>1878</td>\n",
       "      <td>735</td>\n",
       "      <td>1746</td>\n",
       "      <td>0.391374</td>\n",
       "      <td>0.608626</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.929712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>291 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                repo    language  \\\n",
       "2                  DataScienceSpecialization/courses        HTML   \n",
       "4                                   magento/magento2         PHP   \n",
       "5                                    arduino/Arduino        Java   \n",
       "9         bloominstituteoftechnology/Preprocessing-I         CSS   \n",
       "11                                  magento/magento2         PHP   \n",
       "..                                               ...         ...   \n",
       "484                   harshalbenake/hbworkspace1-100        Java   \n",
       "485                                   rprokap/pset-9  JavaScript   \n",
       "486                                 konzy/mass_clone       Shell   \n",
       "487                               biter777/countries          Go   \n",
       "488  sayantann11/all-classification-templetes-for-ML      Python   \n",
       "\n",
       "                                       readme_contents  \\\n",
       "2    \\n### Data Science Specialization\\n\\nThese are...   \n",
       "4    \\n<p align=\"center\">\\n<a href=\"https://www.cod...   \n",
       "5    <p align=\"center\">\\n\\t<img src=\"http://content...   \n",
       "9    # Preprocessing I: Digital Resume\\n\\nFor this ...   \n",
       "11   \\n<p align=\"center\">\\n<a href=\"https://www.cod...   \n",
       "..                                                 ...   \n",
       "484  # hbworkspace1-100\\n\\n(1) Name :- accelormeter...   \n",
       "485  # pset-9\\n      CREDITS SEQUENCE              ...   \n",
       "486  # mass_clone\\nThis is a shell script that will...   \n",
       "487  # countries\\r\\n\\r\\nCountries - ISO 3166 (ISO31...   \n",
       "488  # all-classification-templetes-for-ML\\nClassif...   \n",
       "\n",
       "                                               stemmed  \\\n",
       "2    data scienc specializationthes cours materi jo...   \n",
       "4    magento open sourcewelcom magento open sourc p...   \n",
       "5    &#9; arduino opensourc physic comput platform ...   \n",
       "9    preprocess digit resumefor project build digit...   \n",
       "11   magento open sourcewelcom magento open sourc p...   \n",
       "..                                                 ...   \n",
       "484  hbworkspace11001 name accelormetersensordescri...   \n",
       "485  pset9credit sequencenewspap headlin montagehea...   \n",
       "486  massclonethi shell script clone multipl reposi...   \n",
       "487  countriescountri iso 3166 iso31661 iso3166 dig...   \n",
       "488  allclassificationtempletesformlclassif machin ...   \n",
       "\n",
       "                                            lemmatized  \\\n",
       "2    data science specializationthese course materi...   \n",
       "4    magento open sourcewelcome magento open source...   \n",
       "5    &#9; arduino opensource physical computing pla...   \n",
       "9    preprocessing digital resumefor project buildi...   \n",
       "11   magento open sourcewelcome magento open source...   \n",
       "..                                                 ...   \n",
       "484  hbworkspace11001 name accelormetersensordescri...   \n",
       "485  pset9credits sequencenewspaper headline montag...   \n",
       "486  massclonethis shell script clone multiple repo...   \n",
       "487  countriescountries iso 3166 iso31661 iso3166 d...   \n",
       "488  allclassificationtempletesformlclassification ...   \n",
       "\n",
       "                                             word_list  \\\n",
       "2    [data, science, specializationthese, course, m...   \n",
       "4    [magento, open, sourcewelcome, magento, open, ...   \n",
       "5    [9, arduino, opensource, physical, computing, ...   \n",
       "9    [preprocessing, digital, resumefor, project, b...   \n",
       "11   [magento, open, sourcewelcome, magento, open, ...   \n",
       "..                                                 ...   \n",
       "484  [hbworkspace11001, name, accelormetersensordes...   \n",
       "485  [pset9credits, sequencenewspaper, headline, mo...   \n",
       "486  [massclonethis, shell, script, clone, multiple...   \n",
       "487  [countriescountries, iso, 3166, iso31661, iso3...   \n",
       "488  [allclassificationtempletesformlclassification...   \n",
       "\n",
       "                                          unique_words  \\\n",
       "2    [data, science, specializationthese, course, m...   \n",
       "4    [magento, open, sourcewelcome, source, project...   \n",
       "5    [9, arduino, opensource, physical, computing, ...   \n",
       "9    [preprocessing, digital, resumefor, project, b...   \n",
       "11   [magento, open, sourcewelcome, source, project...   \n",
       "..                                                 ...   \n",
       "484  [hbworkspace11001, name, accelormetersensordes...   \n",
       "485  [pset9credits, sequencenewspaper, headline, mo...   \n",
       "486  [massclonethis, shell, script, clone, multiple...   \n",
       "487  [countriescountries, iso, 3166, iso31661, iso3...   \n",
       "488  [allclassificationtempletesformlclassification...   \n",
       "\n",
       "                                      non_single_words  word_count_simple  \\\n",
       "2    [data, science, course, material, john, data, ...                 36   \n",
       "4    [magento, open, sourcewelcome, magento, open, ...                334   \n",
       "5    [9, arduino, opensource, physical, computing, ...                221   \n",
       "9    [preprocessing, digital, project, building, di...                302   \n",
       "11   [magento, open, sourcewelcome, magento, open, ...                334   \n",
       "..                                                 ...                ...   \n",
       "484  [hbworkspace11001, name, accelormetersensordes...                503   \n",
       "485  [pset9credits, sequencenewspaper, headline, mo...              12801   \n",
       "486  [shell, script, clone, multiple, repositoriest...                106   \n",
       "487  [iso, iso3166, digit, alpha2, alpha3, country,...                536   \n",
       "488  [machine, learning, classification, tutorial, ...               1880   \n",
       "\n",
       "     word_count  unique_count  non_single_count  percent_unique  \\\n",
       "2            36            32                24        0.888889   \n",
       "4           331           218               331        0.658610   \n",
       "5           220           170               168        0.772727   \n",
       "9           293           191               277        0.651877   \n",
       "11          331           218               331        0.658610   \n",
       "..          ...           ...               ...             ...   \n",
       "484         503           361               503        0.717694   \n",
       "485       11927          4354             11927        0.365054   \n",
       "486         106            77                91        0.726415   \n",
       "487         529           226               448        0.427221   \n",
       "488        1878           735              1746        0.391374   \n",
       "\n",
       "     percent_repeat  percent_one_word  percent_non_single  \n",
       "2          0.111111          0.875000            0.666667  \n",
       "4          0.341390          0.784404            1.000000  \n",
       "5          0.227273          0.841176            0.763636  \n",
       "9          0.348123          0.706806            0.945392  \n",
       "11         0.341390          0.784404            1.000000  \n",
       "..              ...               ...                 ...  \n",
       "484        0.282306          0.861496            1.000000  \n",
       "485        0.634946          0.697290            1.000000  \n",
       "486        0.273585          0.779221            0.858491  \n",
       "487        0.572779          0.641593            0.846881  \n",
       "488        0.608626          0.591837            0.929712  \n",
       "\n",
       "[291 rows x 16 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=explore.feature_engineering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0d717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
